{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2e0GYiqPjk"
      },
      "source": [
        "# Homework 3 - Text generation with LSTM and Transformer networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_hn9dHcPKt4"
      },
      "source": [
        "## Installs the unidecode library and downloads the Shakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3qpdcKW580xG"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UG5SIDaHKXC"
      },
      "source": [
        "## LSTM implementation\n",
        "\n",
        "For this task you will implement the LSTM neural network architecture and train it on the task of character-level text generation. Implement a single layer LSTM and optionally extend your implementation to multiple layers to generate better results.\n",
        "\n",
        "Links:\n",
        "\n",
        "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html -- Lists the equations for each component of the LSTM cell.\n",
        "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/ -- Intuitive explanation of LSTM\n",
        "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/ -- Explanation and uses of RNNs.\n",
        "\n",
        "\n",
        "Implement the initialization and the forward pass of a LSTMCell and use it as part of the LSTMSimple network class.\n",
        "\n",
        "The input of the LSTM network will be a sequence of characters, whereas the input of the LSTMCell will be a single input character (x), the output of the previous iteration (C) and the hidden state of the previous iteration (h). Iteratively process the entire input character sequence and calculate the loss based on the prediction at each time step.\n",
        "\n",
        "### Do NOT use the torch.nn.LSTM class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Ma-reelTqEJ7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(LSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # compute the forgetting gate\n",
        "        self.forgetting_gate = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # compute the input gate and new memories\n",
        "        self.input_gate = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.new_memories = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # compute the new hidden state\n",
        "        self.new_hidden_mask = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.new_hidden_vals = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.tensor, C:torch.tensor, h:torch.tensor):\n",
        "        # x - batch of encoded characters\n",
        "        # C - Cell state of the previous iteration\n",
        "        # h - Hidden state of the previous iteration\n",
        "\n",
        "        # concat h_t-1 and x \n",
        "        hidden_stack = torch.concat((x,h), dim=1)\n",
        "        \n",
        "        # calculate forgetting mask\n",
        "        forgetting_mask = self.forgetting_gate(hidden_stack)\n",
        "        # forget some C by the mask\n",
        "        forgotten_C = C * forgetting_mask\n",
        "\n",
        "        # calculat new memories\n",
        "        input_mask = self.input_gate(hidden_stack)\n",
        "        input_vals = self.new_memories(hidden_stack)\n",
        "        masked_new_vals = input_mask * input_vals\n",
        "\n",
        "\n",
        "        # modify cell state with new values\n",
        "        new_C = forgotten_C + masked_new_vals\n",
        "        \n",
        "        # calculat new hidden dim\n",
        "        hiden_mask = self.new_hidden_mask(hidden_stack)\n",
        "        hiden_vals = self.new_hidden_vals(hidden_stack)\n",
        "        new_hidden_state = hiden_mask * hiden_vals\n",
        "\n",
        "        return new_C, new_hidden_state\n",
        "\n",
        "\n",
        "class LSTMSimple(nn.Module):\n",
        "    def __init__(self, seq_length, input_dim, hidden_dim, output_dim, batch_size):\n",
        "        super(LSTMSimple, self).__init__()\n",
        "\n",
        "        self.seq_length = seq_length\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.lstm_cell = LSTMCell(input_dim, hidden_dim, output_dim)\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x - One hot encoded batch - Shape: (batch, seq_len,)\n",
        "        batch, tokens, features = x.shape\n",
        "        c = torch.zeros((batch, self.output_dim)).cuda()\n",
        "        h = torch.zeros((batch, self.hidden_dim)).cuda()\n",
        "        out = torch.zeros((batch, self.seq_length, self.output_dim)).cuda()\n",
        "\n",
        "        for t in range(tokens):\n",
        "            x_t = x[:, t, :]\n",
        "            c, h = self.lstm_cell(x_t,c,h)\n",
        "\n",
        "            o = self.proj(h)\n",
        "            out[:, t,:] = o\n",
        "        \n",
        "        return  out, (c,h)\n",
        "\n",
        "\n",
        "        # Returns the predicted next character for each character in the\n",
        "        # sequence (outputs), also returns the cell state and hidden state of the\n",
        "        # LSTMCell call on the last character. -- outputs, (c,t)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A1_UtJrKnU-"
      },
      "source": [
        "### LSTM Sampling Code\n",
        "\n",
        "To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n",
        "\n",
        "In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n",
        "\n",
        "In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "X23_lg53Kqj2"
      },
      "outputs": [],
      "source": [
        "def greedy_sampling_lstm(lstm, x, num_chars):\n",
        "    # x -- b x onehot_char\n",
        "    outputs = torch.zeros((1,num_chars,x.shape[2]))\n",
        "    t_outputs, (cell_state, hidden) = lstm(x.float())\n",
        "    for c in range(num_chars):\n",
        "        output_tmp = torch.softmax(lstm.proj(hidden),dim=1)\n",
        "        top_ind = torch.argmax(output_tmp,dim=1)[0]\n",
        "        tmp = torch.zeros_like(x[:,0,:]).cuda()\n",
        "        tmp[:,top_ind] = 1\n",
        "        outputs[:,c] = tmp\n",
        "\n",
        "        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n",
        "    return outputs\n",
        "\n",
        "def topk_sampling_lstm(lstm, x, num_chars):\n",
        "    # x -- b x onehot_char\n",
        "    outputs = torch.zeros((1,num_chars,x.shape[2]))\n",
        "    t_outputs, (cell_state, hidden) = lstm(x.float())\n",
        "    for c in range(num_chars):\n",
        "        output_vals, output_ind = torch.topk(lstm.proj(hidden), 5, dim=1)\n",
        "        output_tmp = torch.softmax(output_vals,dim=1)\n",
        "        top_ind = torch.multinomial(output_tmp[0], 1)[0]\n",
        "        tmp = torch.zeros_like(x[:,0,:]).cuda()\n",
        "        tmp[:,output_ind[0,top_ind]] = 1\n",
        "        outputs[:,c] = tmp\n",
        "\n",
        "        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJdxAOVsKzBX"
      },
      "source": [
        "### LSTM Dataset Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "G5U4jzUDK1dG"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class LSTMDataset(Dataset):\n",
        "    def __init__(self, chunk_len=200, padded_chunks=False):\n",
        "        # Character based dataset\n",
        "        dataset_path = \"./input.txt\"\n",
        "        # The tokens in the vocabulary (all_characters)\n",
        "        # are just the printable characters of the string class\n",
        "        self.all_characters = string.printable\n",
        "        self.n_characters = len(self.all_characters)\n",
        "        # Maps characters to indices\n",
        "        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n",
        "        self.file, self.file_len = self.read_file(dataset_path)\n",
        "        # Sequence length of the input\n",
        "        self.chunk_len = chunk_len\n",
        "\n",
        "    def read_file(self,filename):\n",
        "        file = unidecode.unidecode(open(filename).read())\n",
        "        return file, len(file)\n",
        "\n",
        "    def char_tensor(self,in_str):\n",
        "        # in_str - input sequence - String\n",
        "        # Return one-hot encoded characters of in_str\n",
        "        tensor = torch.zeros(len(in_str),self.n_characters).long()\n",
        "        char_ind = [self.char_dict[c] for c in in_str]\n",
        "        tensor[torch.arange(tensor.shape[0]),char_ind] = 1\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target = self.get_random_text()\n",
        "        return {\"input\":inp, \"target\":target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def get_random_text(self):\n",
        "        # Pick a random string of length self.chunk_len from the dataset\n",
        "        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n",
        "        end_index = start_index + self.chunk_len + 1\n",
        "        chunk = self.file[start_index:end_index]\n",
        "        # One-hot encode the chosen string\n",
        "        inp = self.char_tensor(chunk[:-1])\n",
        "        # The target string is the same as the\n",
        "        # input string but shifted by 1 character\n",
        "        target = self.char_tensor(chunk[1:])\n",
        "        inp = Variable(inp).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        return inp, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S7JDlDZRqrg"
      },
      "source": [
        "### LSTM Training loop\n",
        "\n",
        "With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n",
        "especially with the sequence length (chunk_len) used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IDVof1Qe1_vG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30:   3%|▎         | 256/10000 [00:00<00:08, 1180.97chunks/s, loss=4.62, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1160.24chunks/s, loss=2.93, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou  he \n",
            "erit \n",
            " aan \n",
            "h t soun \n",
            "ee ser taee \n",
            " mirerert\n",
            "er sino eo he therese to ne\n",
            " hore\n",
            " a  e\n",
            "tae mor \n",
            "erd mond\n",
            "sha s matt to thase thi  e  oet ta ne\n",
            "\n",
            "\n",
            "o e shas t   a tee\n",
            " t end mo d tee  aon   he shan \n",
            "ha s tho eon \n",
            "ene  ou torot  he done \n",
            "hou \n",
            " mar  oen hhat    eer sat eet eoutt e ners\n",
            " hert\n",
            "\n",
            "hone \n",
            "a\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1095.26chunks/s, loss=2.93, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou  he the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1161.33chunks/s, loss=2.41, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou wor arine waris int therin sheree this, thet an te the sind are hower the meresteres wall thethar and sord ang andend arethan sathe so moterars irith as aretine shand tarestin singert of he myerint ind wous ar ing wer thit thet mares alit we me the the shot, and woun and thee whis,\n",
            "I tises ant oule\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 1/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1098.32chunks/s, loss=2.41, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1119.57chunks/s, loss=2.24, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thoul singe wate ther thale thond sith me peat se thit thee tere, ther at the state, sor that the sore seer the weat hot the thin ther shat tho ghar houn mate the ware ale that ste this at an the ceat tall tite, tot he me herithast ta the coure,, at in ther and he mongen stiss and toust at this wer whes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 2/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1082.07chunks/s, loss=2.24, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thour the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1145.89chunks/s, loss=2.1, lr=0.005, run:=LSTM] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thoug seane his thes is not to thou hom heane and shing ho her, ast well the thin shing ate ho hant to the camy tour shat shald sord se shall th then tha ling than th the hom of\n",
            "With stell wallend, best anghes at oul ser sill we cite to shoredest and the serente, buces inge this seare at ald but him,\n",
            "Th\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 3/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1092.12chunks/s, loss=2.1, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou hat the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1138.05chunks/s, loss=2.02, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art though,\n",
            "Tis tar hom art think the meandst with mis seate hive the ment to mers and thinest all the price, as in the thee fall,\n",
            "And hither soull wingst ang she souss thou monged sersed to mond to to boorse songone thee he prates and have and hemp at made that well the cond is heme thit dintend,\n",
            "In shared\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 4/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1071.53chunks/s, loss=2.02, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou dous the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the sond the so\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1152.61chunks/s, loss=1.91, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art though sene frot make\n",
            "That and mare of hit fay toun beer, teer a well ta bear my herse were how fraid of\n",
            "Tid, are bute the parsunce are to hem to sence in thing thee hered, thy hors, wele hath thin me mighter and ald be antere, singhtre siess.\n",
            "\n",
            "PRONGIO:\n",
            "A dell of my has a fartull at more thom ar ithor o\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 5/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1068.59chunks/s, loss=1.91, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou do the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such and the such a\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1069.23chunks/s, loss=1.86, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou dear the know the men the more and wild brith him thy, buldes trust, this sour him,\n",
            "Will how, be to him seave you so thot she the kinged no the king at you shant thing thou werl with mess that heat,\n",
            "And thougst of your gave my lay, told thougs are an my lord, wher have shard be my look, and the sea\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 6/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1056.04chunks/s, loss=1.86, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou do the stould the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the sear the se\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1151.00chunks/s, loss=1.79, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou a trount in thy she lurest, my larght to this wanger, thought many to man the right to thy lorst this art the roshed.\n",
            "\n",
            "PORINIO:\n",
            "No shall as there is not.\n",
            "\n",
            "BRANUEN:\n",
            "What santing is the preest and mith off tares and me brenged there thene hence of the surder, brind,\n",
            "I have you hishel thise to bear a \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 7/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1085.87chunks/s, loss=1.79, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have the counters and the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the stand the sta\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1177.90chunks/s, loss=1.76, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thoughth with mad mest man me to bleader thou dester him.\n",
            "\n",
            "SAMPLANUS:\n",
            "They goverest best oneredist onde this to morr thee tome, thou wollors and she tiget that tremest as a manters to be true, and stant the kisterthen to stould, mine and, and that hearter have the caute he wather,\n",
            "And ther as holestat h\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 8/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1105.54chunks/s, loss=1.76, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou dost the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the sears the \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1135.52chunks/s, loss=1.7, lr=0.005, run:=LSTM] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou hash a tord are it to here angrouss, but he would the wints, they hast themess.\n",
            "\n",
            "GREMIO:\n",
            "Nay, are to thy were your friath.\n",
            "\n",
            "MENRES:\n",
            "Will, that wouss, beiste so,\n",
            "Be conelless\n",
            "Think not my this done the rager that wo should.\n",
            "\n",
            "GOLIO:\n",
            "I am tithie,\n",
            "Thou a blee a sirs, there this tounts an mold wing of h\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 9/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1099.55chunks/s, loss=1.7, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have a served and the prove the words and the prove the words and the prove the words and the prove the words and the prove the words and the prove the words and the prove the words and the prove the words and the prove the words and the prove the words and the prove the words and the prove the wor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1129.42chunks/s, loss=1.67, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou surdion the stourd, and though dead were a trush, to she this arr my sousies some\n",
            "The stilt and the worncass to deld to do my forench,\n",
            "If years of she he seevy to then, to shate, sir, will sare,\n",
            "I will he deed and but him,\n",
            "That hade the hang to she with his subjeat,\n",
            "And hath show them here.\n",
            "\n",
            "SICINI\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 10/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1098.51chunks/s, loss=1.67, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the promes of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sears of the sear\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1122.47chunks/s, loss=1.63, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou speak.\n",
            "\n",
            "DOKE:\n",
            "Then you shall briok my so fint sone,\n",
            "And I have this would stay this wornt a dost bear by this, his could holl he shring the hower to the suffer all by the stations fall it well think this in some stay.\n",
            "\n",
            "BOUKE:\n",
            "And shall stay is now stint have hear to-marriteds to to her.\n",
            "\n",
            "COMINIUS:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 11/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1083.42chunks/s, loss=1.63, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou shall be the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state the state \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1041.50chunks/s, loss=1.57, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thougs all the words as infall once in so, that we hear's my live a toult bus you this as I be to the sif, there is not to bath his for thain, with all my leass if this is thy pray on all to mean to deep,\n",
            "And so therefild made to-merrion of a sunder time the chereal of all to the promishis\n",
            "Wasting here \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 12/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1075.19chunks/s, loss=1.57, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou shall be the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the prove to the \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1149.20chunks/s, loss=1.57, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou bear.\n",
            "\n",
            "CORIOLANUS:\n",
            "Will husble with hen to this way;\n",
            "I wollors as this my leest,\n",
            "And make me some, beith of his bodes\n",
            "Or shills it is athemble this ass a server'd be of this.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Than I am a cheath, and be though you well,\n",
            "As the world and him tell here, though you arms.\n",
            "\n",
            "KING RICHARD III:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 13/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1072.18chunks/s, loss=1.57, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou shall be the sear the state to the country shall be the sear the state to the country shall be the sear the state to the country shall be the sear the state to the country shall be the sear the state to the country shall be the sear the state to the country shall be the sear the state to the countr\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1182.72chunks/s, loss=1.53, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou hast thee?\n",
            "\n",
            "GRUMIO:\n",
            "I am a cortease their honour. But, sirve the shappary an is alouted with my friends of mine hather sauntion we as he wear to me.\n",
            "\n",
            "GLOUCESTER:\n",
            "That stander tham I lame the colding and him that they was sour than a word you, men you have shouse that thy hands, tire, my forgors:\n",
            "Wh\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 14/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1087.59chunks/s, loss=1.53, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou hast the rest the rest the king of the state of the sentless the sentent the rest and the state of the sentless the sentent the rest and the state of the sentless the sentent the rest and the state of the sentless the sentent the rest and the state of the sentless the sentent the rest and the state\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1171.53chunks/s, loss=1.52, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou them are he doot me to my barks all the wars, the tone of the past o' the world where's the bardens and talk thee words thou denited that he would treath of his foirings,\n",
            "She would never be the fitted that had suct the pears there's a bettard, make his some twath to he drubted to me,\n",
            "I couldon, is \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 15/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1091.23chunks/s, loss=1.52, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou shalt stay the prove to the world the marries the world the marries the world the marries the world the marries the world the marries the world the marries the world the marries the world the marries the world the marries the world the marries the world the marries the world the marries the world t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1175.99chunks/s, loss=1.49, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou hope thou hish poot to my speeting soul will the corrow thee there.\n",
            "\n",
            "LUCIO:\n",
            "I would have stown to be here.\n",
            "\n",
            "KING HENRY VI:\n",
            "This is the cried for some sit, and they stand of the wild with hand that she's hath,\n",
            "To make the childs, and to make heaven, there's the wooness.\n",
            "\n",
            "PETRUCHIO:\n",
            "Andess take thin \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 16/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1092.88chunks/s, loss=1.49, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have a sword and the sund the country's son the command and the country's to the world,\n",
            "And then the world the command and the world,\n",
            "And then the world the command and the world,\n",
            "And then the world the command and the world,\n",
            "And then the world the command and the world,\n",
            "And then the world the comm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1118.92chunks/s, loss=1.5, lr=0.005, run:=LSTM] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou, is she hath news warther hurband,\n",
            "And be thou shall fair with his last is some to his honour than thin hath diserved is the baster and the warling stread of the brothers to him home heart of a powarned, whither or endors,\n",
            "Ans to be a servician and man as deeds of my day!\n",
            "There were thy first,\n",
            "But \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 17/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1086.76chunks/s, loss=1.5, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou hast should be so record and soul the command and the country's son the command and the country's son the command and the country's son the command and the country's son the command and the country's son the command and the country's son the command and the country's son the command and the country\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1155.35chunks/s, loss=1.47, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou say,\n",
            "This is as these stander transof liggly death.\n",
            "\n",
            "KING RICHARD III:\n",
            "Make a sweet some, siles,\n",
            "Why, thou's the surst\n",
            "To perter the day a hard,\n",
            "Too hath the dost, thou strittle.\n",
            "\n",
            "COMINIUS:\n",
            "Why what all a service here,\n",
            "I'll there a burged to hath for a must belight this true.\n",
            "\n",
            "BENVOLIO:\n",
            "To making h\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 18/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1090.38chunks/s, loss=1.47, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou hast the world,\n",
            "That we may be so the married the strength,\n",
            "That we may be so the married the strength,\n",
            "That we may be so the married the strength,\n",
            "That we may be so the married the strength,\n",
            "That we may be so the married the strength,\n",
            "That we may be so the married the strength,\n",
            "That we may be so t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1163.73chunks/s, loss=1.45, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou hast me but to-morrow to and the preasing\n",
            "Agone,\n",
            "To see thy since anot thou art,--\n",
            "\n",
            "Richar:\n",
            "Is thy sound whole that will\n",
            "To be a most buin to them to mare. The man it so, there, when,\n",
            "Is thy for thee.\n",
            "\n",
            "POMPEY:\n",
            "I was nother.\n",
            "\n",
            "MISTRENNE:\n",
            "Who sour heart, my fortune than any as they to\n",
            "the sends.\n",
            "\n",
            "COMI\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 19/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1092.12chunks/s, loss=1.45, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the state,\n",
            "And there is a soldier to the sunters and the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun the sun th\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1142.16chunks/s, loss=1.46, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou whom thy honour,\n",
            "To more than we see my sovereign men:\n",
            "Then, and so soul abood. Thy father and serves how that, I'll stays, and they true monoth to make my possare and the wife,\n",
            "The brother with the tangard,\n",
            "In pains. What, if it be talk the tame,\n",
            "I was the cloods,\n",
            "And there word made a persead, al\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 20/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1073.25chunks/s, loss=1.46, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou hast thou shall be so to the prince the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the sease the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1122.01chunks/s, loss=1.42, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou have a should he his down thy fore against a friar,\n",
            "And nays say that I had\n",
            "He shall therefore that he his engething that an all.\n",
            "\n",
            "KING RENCEPEY:\n",
            "Your changed to be so.\n",
            "\n",
            "CORIOLANUS:\n",
            "Have, worsh our hand to speak and toneral\n",
            "he heard the while I him a words that speching treams;\n",
            "Frunt it, be not see\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 21/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1073.45chunks/s, loss=1.42, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou hast thou the world that the world the sen to the world that the world the sen to the world that the world the sen to the world that the world the sen to the world that the world the sen to the world that the world the sen to the world that the world the sen to the world that the world the sen to t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1156.81chunks/s, loss=1.43, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou, the wish any sign of a bear how\n",
            "Or sea strung himself here burn to deep than hears to my fault\n",
            "Then strokents for his son tell and time of hath find,\n",
            "The gracessish'd and have much makes a posce last shall have the than trumpets.\n",
            "\n",
            "LADY ANNE:\n",
            "Why, hast thou wilt for my counternice of my love;\n",
            "And t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 22/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1087.58chunks/s, loss=1.43, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the words and the good to the world and the good to the world and the good to the world and the good to the world and the good to the world and the good to the world and the good to the world and the good to the world and the good to the world and the good to the world and the good to the world\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1117.74chunks/s, loss=1.4, lr=0.005, run:=LSTM] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou shalt should break'st brother will to to mine, and the words. Clarities.\n",
            "\n",
            "KING RICHARD III:\n",
            "A petit him of the sur,\n",
            "Ay.\n",
            "My free my free my signing to thy son were all to my stands\n",
            "The crown is that would tale her son, I am this better bleath the stame that them hark,\n",
            "I will their streathest, and wi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 23/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1052.25chunks/s, loss=1.4, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou shalt be so far as the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and the death and \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1151.48chunks/s, loss=1.4, lr=0.005, run:=LSTM] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou, and. What conscience a tile some fair and her\n",
            "shep a shale, that so such me and the conquirence in them;\n",
            "In that thou wilt a tortal diech of his execution withis\n",
            "Till house when he stall\n",
            "Fear you the prince his strember.\n",
            "\n",
            "Clown:\n",
            "I will to thy hand,\n",
            "True time himself.-\n",
            "Both true my goant.\n",
            "\n",
            "LUCENTIO\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 24/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1098.80chunks/s, loss=1.4, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou art the sun.\n",
            "\n",
            "CORIOLANUS:\n",
            "What shall be so fare and the seas and the sun of the world,\n",
            "And the prince and the sun of the world,\n",
            "And the prince and the sun of the world,\n",
            "And the prince and the sun of the world,\n",
            "And the prince and the sun of the world,\n",
            "And the prince and the sun of the world,\n",
            "And the\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1155.44chunks/s, loss=1.38, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou art\n",
            "True--\n",
            "\n",
            "KI GoE:\n",
            "The plood and businems,\n",
            "Inlest thou, to have you, so long again. I will be so it is son,\n",
            "So her him to be most good for thy bready:\n",
            "Thy holours baugh at to my husband\n",
            "That I will see that him,\n",
            "I am not there,\n",
            "We cannot do in thee, are thou have\n",
            "I would hover a down.\n",
            "\n",
            "DUKE OF YOR\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 25/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1086.69chunks/s, loss=1.38, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou be the world and the sen to the senous and the sen to the senous and the sen to the senous and the sen to the senous and the sen to the senous and the sen to the senous and the sen to the senous and the sen to the senous and the sen to the senous and the sen to the senous and the sen to the senous \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1164.18chunks/s, loss=1.42, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou, and have you bear his begghence,\n",
            "Though it is mother and at the sin will not, sister of a scall.\n",
            "\n",
            "LEONTES:\n",
            "Tell yet he is, mine eyes of this woold, that I al acts and they so much and what thou hast make me thy more with the precealy it.\n",
            "\n",
            "CLow:\n",
            "My brows of continulance, and with many.\n",
            "\n",
            "LEONTESO:\n",
            "A\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 26/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1103.74chunks/s, loss=1.42, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou with the sun to the world,\n",
            "And then the great double and a prince the law from the common for the sun\n",
            "Which is the sun to the world and the secret and the sun to the world,\n",
            "And then the great double and a prince the law from the common for the sun\n",
            "Which is the sun to the world and the secret and th\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1166.78chunks/s, loss=1.4, lr=0.005, run:=LSTM] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou some mine,\n",
            "And me that said wishom or that he doth.\n",
            "\n",
            "Second Murderer:\n",
            "I have done without\n",
            "The woundly father, a man be say nor his still,\n",
            "And there that honest that take hearing all myself\n",
            "Would hath but there?\n",
            "\n",
            "Provost:\n",
            "He seem' to me;\n",
            "And then whom I am part and seen of the service.\n",
            "\n",
            "CORIOL:\n",
            "We a\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 27/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1089.78chunks/s, loss=1.4, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou have seen the world in the world,\n",
            "And there is a word and heart that show the prince and so strange of the world,\n",
            "And there is a word and heart that show the prince and so strange of the world,\n",
            "And there is a word and heart that show the prince and so strange of the world,\n",
            "And there is a word and h\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1161.41chunks/s, loss=1.35, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou whom I have tening mine\n",
            "Whom thou art thou dost thou whom will'st this,\n",
            "In they are to her:\n",
            "In the sore to him he they are this itself.\n",
            "\n",
            "POLIXENES:\n",
            "That I must break too\n",
            "Trunt and to thy pains:\n",
            "If this bear his seen of thee, and that say that I did monest our curse your child; and see thee\n",
            "I devil \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 28/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1091.81chunks/s, loss=1.35, lr=0.005, run:=LSTM]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou worthy son, and the state and the sun the rest to be married to his court, and the sun the rest to be married to his court, and the sun the rest to be married to his court, and the sun the rest to be married to his court, and the sun the rest to be married to his court, and the sun the rest to be m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:08<00:00, 1158.24chunks/s, loss=1.36, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling -----------------\n",
            "O Romeo, wherefore art thou, Is there soul to plament,\n",
            "And they do nature\n",
            "In sometitions.\n",
            "\n",
            "COMINIUS:\n",
            "Therefore for him hence taskel and\n",
            "Thou art.\n",
            "\n",
            "PETRUCHIO:\n",
            "You shall play the way.\n",
            "I call he dowly son are, and by you.\n",
            "\n",
            "First Catizen:\n",
            "His wrong'd thy hope,\n",
            "If you have been a part,\n",
            "But tell the words, sir!\n",
            "\n",
            "PAULINA:\n",
            "No, nake a\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 29/30: 100%|█████████▉| 9984/10000 [00:09<00:00, 1090.13chunks/s, loss=1.36, lr=0.005, run:=LSTM]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy sampling ----------------\n",
            "O Romeo, wherefore art thou be the world and the seatent of the world and the seatent of the world and the seatent of the world and the seatent of the world and the seatent of the world and the seatent of the world and the seatent of the world and the seatent of the world and the seatent of the world and the seatent of the wo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "batch_size = 256\n",
        "chunk_len = 128\n",
        "model_name = \"LSTM\"\n",
        "train_dataset = LSTMDataset(chunk_len=chunk_len)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, drop_last=True)\n",
        "\n",
        "#Sample parameters, use whatever you see fit.\n",
        "input_dim = train_dataset.n_characters\n",
        "hidden_dim = 256\n",
        "output_dim = train_dataset.n_characters\n",
        "learning_rate = 0.005\n",
        "model = LSTMSimple(chunk_len,input_dim, hidden_dim, output_dim,batch_size)\n",
        "model.train()\n",
        "model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs=30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs = data['input'].float()\n",
        "            labels = data['target'].float()\n",
        "            # b x chunk_len x len(dataset.all_characters)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            target = torch.argmax(labels,dim=2)\n",
        "\n",
        "            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target.view(labels.shape[0]*labels.shape[1]))\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
        "                                      max_norm=10.0)\n",
        "            optimizer.step()\n",
        "            prog_bar.set_postfix(**{'run:': model_name,'lr': learning_rate,\n",
        "                                    'loss': loss.item()\n",
        "                                    })\n",
        "            prog_bar.update(batch_size)\n",
        "        # Intermediate output\n",
        "        sample_text = \"O Romeo, wherefore art thou\"\n",
        "        inp = train_dataset.char_tensor(sample_text)\n",
        "        sample_input = Variable(inp).cuda().unsqueeze(0).float()\n",
        "        out_test = topk_sampling_lstm(model,sample_input, 300)[0]\n",
        "        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n",
        "        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "        print(\"Top-K sampling -----------------\")\n",
        "        print(out_chars)\n",
        "\n",
        "        out_test = greedy_sampling_lstm(model,sample_input, 300)[0]\n",
        "        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n",
        "        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "        print(\"Greedy sampling ----------------\")\n",
        "        print(out_chars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwQEM1JHzDo-"
      },
      "source": [
        "# Task 2: Character generation transformer network implementation\n",
        "Our simple transformer-like network will take as input a sequence of characters and predict the next character in the sequence. To ensure an efficient training procedure, masked attention modules will be used as in the [GPT model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
        "\n",
        "For this task you must implement the Scaled dot product attention module and the Masked multi-head attention module. Both of these modules are described in the [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper (See Figure 2 in the paper as well as Sections 3.2.1, 3.2.2 and 3.2.3). They are the core operations of transformers. As we will use our model for text generation also add the masking operation shown as (mask opt.) in Figure 2, implemented as AttentionMasking in the code.\n",
        "\n",
        "**Implement the modules in the ScaledDotProductAttention class and the MultiHeadAttention class.**\n",
        "\n",
        "Read the GPT paper and the Attention is all you need paper for a better understanding of the components. For a more high level overview, this [post](https://jalammar.github.io/illustrated-gpt2/) may also be helpful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lNhqnTm8zGRe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "        # Positional encoding adds the positional information to the\n",
        "        # embedding. Without it the model would not be able to differentiate\n",
        "        # between different characters orders such as between \"dog\" and \"god\".\n",
        "        position = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div_term = 10000.0**(torch.arange(0,d_model,2).float()/d_model)\n",
        "        print(div_term.shape)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position / div_term)\n",
        "        pe[:, 1::2] = torch.cos(position / div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.pe = pe.cuda()\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.pe[:, :x.size(1)]\n",
        "        return p\n",
        "\n",
        "class AttentionMasking(nn.Module):\n",
        "    def __init__(self, max_len):\n",
        "        super(AttentionMasking, self).__init__()\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_len, max_len))\n",
        "                                     .view(1, 1, max_len, max_len))\n",
        "    def forward(self,x):\n",
        "        length = x.shape[-1]\n",
        "        out = x.masked_fill(self.mask[:,:,:length,:length] == 0, float('-inf'))\n",
        "        return out\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, max_len):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        # Multiply with an upper triangular\n",
        "        # matrix of dimensions (length x length) after the scale operation\n",
        "        # in Figure 2 of the paper.\n",
        "        self.mask_opt = AttentionMasking(max_len)\n",
        "\n",
        "\n",
        "    def forward(self,q,k,v):\n",
        "      # length = number of input tokens\n",
        "      batch_size,num_heads,length,num_neuron = k.size()\n",
        "      # TODO: Implement the scaled dot product attention as described in\n",
        "      # the Attention is all you need paper in Equation 1\n",
        "      pass\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim_model, num_neuron, n_head, max_len):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.num_neuron = num_neuron\n",
        "\n",
        "        # TODO: Initialize the ScaledDotProductAttention and other\n",
        "        # necessary components.\n",
        "\n",
        "    def split(self,tensor):\n",
        "        batch_size, length, total_dim = tensor.size()\n",
        "        # Reshape the tensor to enable the use in\n",
        "        # the ScaledDotProductAttention module\n",
        "        split_tensor = tensor.view(batch_size, length, self.n_head, self.num_neuron).transpose(1,2)\n",
        "        return split_tensor\n",
        "\n",
        "    def concat(self,tensor):\n",
        "        batch_size, num_heads, length, num_neuron = tensor.size()\n",
        "        # Reshape the tensor to its original size before the split operation.\n",
        "        concat_tensor = tensor.transpose(1,2).contiguous().view(batch_size, length, self.n_head*self.num_neuron)\n",
        "        return concat_tensor\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # TODO: Implement the Masked Multi-head attention module as described in the\n",
        "        # Attention is all you need paper in Figure 1 and Section 3.2.2.\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "class PositionFeedForwardNet(nn.Module):\n",
        "    def __init__(self, dim_model):\n",
        "        super(PositionFeedForwardNet, self).__init__()\n",
        "        self.ff_net1 = nn.Linear(dim_model, dim_model*4)\n",
        "        self.ff_net2 = nn.Linear(dim_model*4, dim_model)\n",
        "    def forward(self,x):\n",
        "        ff_out = self.ff_net1(x)\n",
        "        ff_out = torch.nn.functional.relu(ff_out)\n",
        "        ff_out = self.ff_net2(ff_out)\n",
        "        return ff_out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim_model, num_neuron, n_head, max_len):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(dim_model, num_neuron, n_head, max_len)\n",
        "        self.l_norm = torch.nn.LayerNorm(dim_model)\n",
        "        self.l_norm2 = torch.nn.LayerNorm(dim_model)\n",
        "        self.ff_net = PositionFeedForwardNet(dim_model)\n",
        "        # b, len_seq, n_head, num_neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "      # A Transformer block as described in the\n",
        "      # Attention is all you need paper. In Figure 1 the transformer\n",
        "      # block is marked with a gray rectangle right of the text \"Nx\"\n",
        "      _x = x\n",
        "      mha1 = self.mha(x,x,x)\n",
        "      lnorm = self.l_norm(_x+mha1)\n",
        "      _x = lnorm\n",
        "      ff_out = self.ff_net(lnorm)\n",
        "      out = self.l_norm2(ff_out+_x)\n",
        "\n",
        "      return out\n",
        "\n",
        "class TransformerSimple(nn.Module):\n",
        "    def __init__(self, seq_length, input_dim, output_dim,\n",
        "                 batch_size):\n",
        "        super(TransformerSimple, self).__init__()\n",
        "        num_neuron = 64\n",
        "        n_head = 8\n",
        "        dim_model=256\n",
        "        max_len = 512\n",
        "        self.start_embedding = nn.Embedding(input_dim, dim_model)\n",
        "\n",
        "        self.pos_embedding = PositionalEncoding(dim_model)\n",
        "\n",
        "        # b x l x c*n_head\n",
        "        self.t_block1 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block2 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block3 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block4 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block5 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "\n",
        "        #self.out_layer_1 = nn.Linear(dim_model, dim_model)\n",
        "        self.output_layer = nn.Linear(dim_model,output_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "      # x - Tensor - (b, seq_len)\n",
        "      # Embeds the input tensor from tokens to features\n",
        "      s_emb = self.start_embedding(x)\n",
        "      # Adds positional embeddings\n",
        "      p_emb = self.pos_embedding(s_emb)\n",
        "      b_out = p_emb + s_emb\n",
        "      # Transformer blocks - You can experiment with varying depth\n",
        "      # For example GPT uses 12 blocks but this might be a bit memory intensive\n",
        "      b_out = self.t_block1(b_out)\n",
        "      b_out = self.t_block2(b_out)\n",
        "      b_out = self.t_block3(b_out)\n",
        "      b_out = self.t_block4(b_out)\n",
        "      b_out = self.t_block5(b_out)\n",
        "\n",
        "      # Output mapping to a classification of output tokens\n",
        "      # For each token the network tries to predict the next token\n",
        "      # based only on the previous tokens.\n",
        "      # Output shape: (b x seq_len x vocabulary_size)\n",
        "      out = self.output_layer(b_out)\n",
        "\n",
        "      return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPjl6ttzPHsq"
      },
      "source": [
        "## Dataset class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "T04eYePr8Gn3"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, chunk_len=200, padded_chunks=False):\n",
        "        # Character based dataset\n",
        "        dataset_path = \"./input.txt\"\n",
        "        # The tokens in the vocabulary (all_characters)\n",
        "        # are just the printable characters of the string class\n",
        "        self.all_characters = string.printable\n",
        "        self.n_characters = len(self.all_characters)\n",
        "        # Maps characters to indices\n",
        "        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n",
        "        self.file, self.file_len = self.read_file(dataset_path)\n",
        "        # Sequence length of the input\n",
        "        self.chunk_len = chunk_len\n",
        "        self.encoded_file = [self.char_dict[x] for x in self.file]\n",
        "\n",
        "    def read_file(self,filename):\n",
        "        file = unidecode.unidecode(open(filename).read())\n",
        "        return file, len(file)\n",
        "\n",
        "    def encode_text(self,in_str):\n",
        "        # in_str - input sequence - String\n",
        "        # Returns - in_str mapped to tokens in char_dict\n",
        "        tensor = torch.LongTensor([self.char_dict[x] for x in in_str])\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target = self.get_random_text()\n",
        "        return {\"input\":inp, \"target\":target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def get_random_text(self):\n",
        "        # Pick a random string of length self.chunk_len from the dataset\n",
        "        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n",
        "        end_index = start_index + self.chunk_len + 1\n",
        "        chunk = self.encoded_file[start_index:end_index]\n",
        "        # input_tokens - random sequence of tokens from the dataset\n",
        "        input_tokens = torch.LongTensor(chunk[:-1])\n",
        "        # target - input token sequence shifted by 1\n",
        "        # the idea is to predict next token for each token in the input sequence\n",
        "        # therefore if the input is [1,2,3,4] the target is [2,3,4,5]\n",
        "        target = torch.LongTensor(chunk[1:])\n",
        "        input_tokens = input_tokens.cuda()\n",
        "        target = target.cuda()\n",
        "        return input_tokens, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhvayVAjsCMf"
      },
      "source": [
        "## Character sampling\n",
        "\n",
        "To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n",
        "\n",
        "In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n",
        "\n",
        "In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3IVliOUqqEd5"
      },
      "outputs": [],
      "source": [
        "def topk_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n",
        "    # x -- b x onehot_char\n",
        "    # x = b x l\n",
        "    outputs = torch.zeros((1,num_chars))\n",
        "    inp = x\n",
        "\n",
        "    for t in range(num_chars):\n",
        "        # b x onehot_char\n",
        "        output = model(inp.long())[0,-1:]\n",
        "        #output = torch.softmax(output, dim=1)\n",
        "        # b x 3\n",
        "        output_vals, output_ind = torch.topk(output, 5, dim=1)\n",
        "        # 3 -> int\n",
        "        output_vals = torch.softmax(output_vals, dim=1)\n",
        "        top_ind = torch.multinomial(output_vals[0], 1)[0]\n",
        "        # int\n",
        "        out_char_index = output_ind[0,top_ind]\n",
        "        # int -> 1\n",
        "        out_char_index = torch.ones(1).cuda() * out_char_index\n",
        "\n",
        "        outputs[:,t] = out_char_index.item()\n",
        "        if inp.shape[1] > chunk_len:\n",
        "          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n",
        "        else:\n",
        "          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def greedy_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n",
        "    # x -- shape (batch, tokens in x)\n",
        "    outputs = torch.zeros((1,num_chars))\n",
        "    inp = x\n",
        "\n",
        "    for t in range(num_chars):\n",
        "        # b x l x onehot_char\n",
        "        output = model(inp.long())[0,-1:]\n",
        "        output = torch.softmax(output, dim=1)\n",
        "        out_char_index = torch.argmax(output, dim=1)\n",
        "        outputs[:,t] = out_char_index.item()\n",
        "        if inp.shape[1] > chunk_len:\n",
        "          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n",
        "        else:\n",
        "          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6X7-Tlc2iqh"
      },
      "source": [
        "## Transformer model training\n",
        "\n",
        "With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n",
        "especially with the sequence length (chunk_len) used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "gY8aZz1R2g3M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/50:   0%|          | 0/10000 [00:00<?, ?chunks/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'Tensor' and 'NoneType'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m target_t \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39minputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),target_t\u001b[38;5;241m.\u001b[39mview(labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mlabels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n",
            "File \u001b[0;32m~/Faks/mag/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Faks/mag/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[40], line 151\u001b[0m, in \u001b[0;36mTransformerSimple.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m b_out \u001b[38;5;241m=\u001b[39m p_emb \u001b[38;5;241m+\u001b[39m s_emb\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Transformer blocks - You can experiment with varying depth\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# For example GPT uses 12 blocks but this might be a bit memory intensive\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m b_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_block1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m b_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_block2(b_out)\n\u001b[1;32m    153\u001b[0m b_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_block3(b_out)\n",
            "File \u001b[0;32m~/Faks/mag/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Faks/mag/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[40], line 113\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m _x \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    112\u001b[0m mha1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha(x,x,x)\n\u001b[0;32m--> 113\u001b[0m lnorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml_norm(\u001b[43m_x\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmha1\u001b[49m)\n\u001b[1;32m    114\u001b[0m _x \u001b[38;5;241m=\u001b[39m lnorm\n\u001b[1;32m    115\u001b[0m ff_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_net(lnorm)\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Tensor' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#Sample parameters, use whatever you see fit.\n",
        "batch_size = 256\n",
        "chunk_len = 128\n",
        "train_dataset = TextDataset(chunk_len=chunk_len)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "input_dim = train_dataset.n_characters\n",
        "output_dim = train_dataset.n_characters\n",
        "learning_rate = 0.0006\n",
        "\n",
        "model = TransformerSimple(chunk_len, input_dim, output_dim,batch_size)\n",
        "model.train()\n",
        "model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs=50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # inputs - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n",
        "            inputs = data['input'].long()\n",
        "            # labels - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n",
        "            labels = data['target'].long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            target_t = labels\n",
        "            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target_t.view(labels.shape[0]*labels.shape[1]))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            prog_bar.set_postfix(**{'run:': \"Transformer\", 'lr': learning_rate,\n",
        "                                    'loss': loss.item()\n",
        "                                    })\n",
        "            prog_bar.update(batch_size)\n",
        "\n",
        "        # Intermediate text output\n",
        "        sample_texts = [\"What authority surfeits on\",\n",
        "                        \"I say unto you, what he hath done famously, he did it to that end:\",\n",
        "                        \"That in submission will return to us: And then, as we have ta'en the sacrament,\"]\n",
        "        output_token = torch.zeros(1,1).cuda()\n",
        "        output_token[0,0] = train_dataset.n_characters-1\n",
        "        print(\"Top-K sampling\")\n",
        "        for sample_text in sample_texts:\n",
        "            sample_encoding = train_dataset.encode_text(sample_text)\n",
        "            sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "\n",
        "            #out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "            out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "            out_char_index = out_test.long().detach().cpu().numpy()\n",
        "            out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "            print(\"----------------------------------------\")\n",
        "            print(out_chars)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-Tyjm1kHz5Z"
      },
      "source": [
        "## Text sampling - Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjvbjEdjH36Q"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "sample_text = \"Here's to my love! O true apothecary! Thy drugs are quick.\"\n",
        "sample_encoding = train_dataset.encode_text(sample_text)\n",
        "sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "\n",
        "#out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "out_char_index = out_test.long().detach().cpu().numpy()\n",
        "out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "print(\"----------------------------------------\")\n",
        "print(out_chars)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
