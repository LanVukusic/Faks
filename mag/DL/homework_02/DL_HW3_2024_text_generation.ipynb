{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2e0GYiqPjk"
      },
      "source": [
        "# Homework 3 - Text generation with LSTM and Transformer networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_hn9dHcPKt4"
      },
      "source": [
        "## Installs the unidecode library and downloads the Shakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "3qpdcKW580xG"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UG5SIDaHKXC"
      },
      "source": [
        "## LSTM implementation\n",
        "\n",
        "For this task you will implement the LSTM neural network architecture and train it on the task of character-level text generation. Implement a single layer LSTM and optionally extend your implementation to multiple layers to generate better results.\n",
        "\n",
        "Links:\n",
        "\n",
        "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html -- Lists the equations for each component of the LSTM cell.\n",
        "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/ -- Intuitive explanation of LSTM\n",
        "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/ -- Explanation and uses of RNNs.\n",
        "\n",
        "\n",
        "Implement the initialization and the forward pass of a LSTMCell and use it as part of the LSTMSimple network class.\n",
        "\n",
        "The input of the LSTM network will be a sequence of characters, whereas the input of the LSTMCell will be a single input character (x), the output of the previous iteration (C) and the hidden state of the previous iteration (h). Iteratively process the entire input character sequence and calculate the loss based on the prediction at each time step.\n",
        "\n",
        "### Do NOT use the torch.nn.LSTM class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Ma-reelTqEJ7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(LSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # compute the forgetting gate\n",
        "        self.forgetting_gate = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # compute the input gate and new memories\n",
        "        self.input_gate = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.new_memories = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # compute the new hidden state\n",
        "        self.new_hidden_mask = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.new_hidden_vals = nn.Sequential(\n",
        "            nn.Linear(self.input_dim+self.hidden_dim, self.hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.tensor, C:torch.tensor, h:torch.tensor):\n",
        "        # x - batch of encoded characters\n",
        "        # C - Cell state of the previous iteration\n",
        "        # h - Hidden state of the previous iteration\n",
        "\n",
        "        # concat h_t-1 and x \n",
        "        hidden_stack = torch.concat((x,h), dim=1)\n",
        "        \n",
        "        # calculate forgetting mask\n",
        "        forgetting_mask = self.forgetting_gate(hidden_stack)\n",
        "        # forget some C by the mask\n",
        "        forgotten_C = C * forgetting_mask\n",
        "\n",
        "        # calculat new memories\n",
        "        input_mask = self.input_gate(hidden_stack)\n",
        "        input_vals = self.new_memories(hidden_stack)\n",
        "        masked_new_vals = input_mask * input_vals\n",
        "\n",
        "\n",
        "        # modify cell state with new values\n",
        "        new_C = forgotten_C + masked_new_vals\n",
        "        \n",
        "        # calculat new hidden dim\n",
        "        hiden_mask = self.new_hidden_mask(hidden_stack)\n",
        "        hiden_vals = self.new_hidden_vals(hidden_stack)\n",
        "        new_hidden_state = hiden_mask * hiden_vals\n",
        "\n",
        "        return new_C, new_hidden_state\n",
        "\n",
        "\n",
        "class LSTMSimple(nn.Module):\n",
        "    def __init__(self, seq_length, input_dim, hidden_dim, output_dim, batch_size):\n",
        "        super(LSTMSimple, self).__init__()\n",
        "\n",
        "        self.seq_length = seq_length\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.lstm_cell = LSTMCell(input_dim, hidden_dim, output_dim)\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x - One hot encoded batch - Shape: (batch, seq_len,)\n",
        "        batch, tokens, features = x.shape\n",
        "        c = torch.zeros((batch, self.output_dim)).cuda()\n",
        "        h = torch.zeros((batch, self.hidden_dim)).cuda()\n",
        "        out = torch.zeros((batch, self.seq_length, self.output_dim)).cuda()\n",
        "\n",
        "        for t in range(tokens):\n",
        "            x_t = x[:, t, :]\n",
        "            c, h = self.lstm_cell(x_t,c,h)\n",
        "\n",
        "            o = self.proj(h)\n",
        "            out[:, t,:] = o\n",
        "        \n",
        "        return  out, (c,h)\n",
        "\n",
        "\n",
        "        # Returns the predicted next character for each character in the\n",
        "        # sequence (outputs), also returns the cell state and hidden state of the\n",
        "        # LSTMCell call on the last character. -- outputs, (c,t)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A1_UtJrKnU-"
      },
      "source": [
        "### LSTM Sampling Code\n",
        "\n",
        "To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n",
        "\n",
        "In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n",
        "\n",
        "In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "X23_lg53Kqj2"
      },
      "outputs": [],
      "source": [
        "def greedy_sampling_lstm(lstm, x, num_chars):\n",
        "    # x -- b x onehot_char\n",
        "    outputs = torch.zeros((1,num_chars,x.shape[2]))\n",
        "    t_outputs, (cell_state, hidden) = lstm(x.float())\n",
        "    for c in range(num_chars):\n",
        "        output_tmp = torch.softmax(lstm.proj(hidden),dim=1)\n",
        "        top_ind = torch.argmax(output_tmp,dim=1)[0]\n",
        "        tmp = torch.zeros_like(x[:,0,:]).cuda()\n",
        "        tmp[:,top_ind] = 1\n",
        "        outputs[:,c] = tmp\n",
        "\n",
        "        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n",
        "    return outputs\n",
        "\n",
        "def topk_sampling_lstm(lstm, x, num_chars):\n",
        "    # x -- b x onehot_char\n",
        "    outputs = torch.zeros((1,num_chars,x.shape[2]))\n",
        "    t_outputs, (cell_state, hidden) = lstm(x.float())\n",
        "    for c in range(num_chars):\n",
        "        output_vals, output_ind = torch.topk(lstm.proj(hidden), 5, dim=1)\n",
        "        output_tmp = torch.softmax(output_vals,dim=1)\n",
        "        top_ind = torch.multinomial(output_tmp[0], 1)[0]\n",
        "        tmp = torch.zeros_like(x[:,0,:]).cuda()\n",
        "        tmp[:,output_ind[0,top_ind]] = 1\n",
        "        outputs[:,c] = tmp\n",
        "\n",
        "        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJdxAOVsKzBX"
      },
      "source": [
        "### LSTM Dataset Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "G5U4jzUDK1dG"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class LSTMDataset(Dataset):\n",
        "    def __init__(self, chunk_len=200, padded_chunks=False):\n",
        "        # Character based dataset\n",
        "        dataset_path = \"./input.txt\"\n",
        "        # The tokens in the vocabulary (all_characters)\n",
        "        # are just the printable characters of the string class\n",
        "        self.all_characters = string.printable\n",
        "        self.n_characters = len(self.all_characters)\n",
        "        # Maps characters to indices\n",
        "        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n",
        "        self.file, self.file_len = self.read_file(dataset_path)\n",
        "        # Sequence length of the input\n",
        "        self.chunk_len = chunk_len\n",
        "\n",
        "    def read_file(self,filename):\n",
        "        file = unidecode.unidecode(open(filename).read())\n",
        "        return file, len(file)\n",
        "\n",
        "    def char_tensor(self,in_str):\n",
        "        # in_str - input sequence - String\n",
        "        # Return one-hot encoded characters of in_str\n",
        "        tensor = torch.zeros(len(in_str),self.n_characters).long()\n",
        "        char_ind = [self.char_dict[c] for c in in_str]\n",
        "        tensor[torch.arange(tensor.shape[0]),char_ind] = 1\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target = self.get_random_text()\n",
        "        return {\"input\":inp, \"target\":target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def get_random_text(self):\n",
        "        # Pick a random string of length self.chunk_len from the dataset\n",
        "        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n",
        "        end_index = start_index + self.chunk_len + 1\n",
        "        chunk = self.file[start_index:end_index]\n",
        "        # One-hot encode the chosen string\n",
        "        inp = self.char_tensor(chunk[:-1])\n",
        "        # The target string is the same as the\n",
        "        # input string but shifted by 1 character\n",
        "        target = self.char_tensor(chunk[1:])\n",
        "        inp = Variable(inp).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        return inp, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S7JDlDZRqrg"
      },
      "source": [
        "### LSTM Training loop\n",
        "\n",
        "With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n",
        "especially with the sequence length (chunk_len) used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "batch_size = 256\n",
        "chunk_len = 128\n",
        "model_name = \"LSTM\"\n",
        "train_dataset = LSTMDataset(chunk_len=chunk_len)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, drop_last=True)\n",
        "LSTM_MODEL_PATH = f\"{model_name}-b{batch_size}-ch{chunk_len}.cktp\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "IDVof1Qe1_vG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30:   0%|          | 0/10000 [00:00<?, ?chunks/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/30:   0%|          | 0/10000 [00:00<?, ?chunks/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[77], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# b x chunk_len x len(dataset.all_characters)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(labels,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39minputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),target\u001b[38;5;241m.\u001b[39mview(labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mlabels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n",
            "File \u001b[0;32m~/Faks/mag/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Faks/mag/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[71], line 88\u001b[0m, in \u001b[0;36mLSTMSimple.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# x - One hot encoded batch - Shape: (batch, seq_len,)\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     batch, tokens, features \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     89\u001b[0m     c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     90\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim))\u001b[38;5;241m.\u001b[39mcuda()\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "\n",
        "#Sample parameters, use whatever you see fit.\n",
        "input_dim = train_dataset.n_characters\n",
        "hidden_dim = 256\n",
        "output_dim = train_dataset.n_characters\n",
        "learning_rate = 0.005\n",
        "model = LSTMSimple(chunk_len,input_dim, hidden_dim, output_dim,batch_size)\n",
        "model.train()\n",
        "model.cuda()\n",
        "\n",
        "# if(os.path.exists(LSTM_MODEL_PATH)):\n",
        "#     model.load_state_dict(torch.load(LSTM_MODEL_PATH))\n",
        "# else:\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "epochs=30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs = data['input'].float()\n",
        "            labels = data['target'].float()\n",
        "            # b x chunk_len x len(dataset.all_characters)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            target = torch.argmax(labels,dim=2)\n",
        "\n",
        "            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target.view(labels.shape[0]*labels.shape[1]))\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
        "                                    max_norm=10.0)\n",
        "            optimizer.step()\n",
        "            prog_bar.set_postfix(**{'run:': model_name,'lr': learning_rate,\n",
        "                                    'loss': loss.item()\n",
        "                                    })\n",
        "            prog_bar.update(batch_size)\n",
        "        # Intermediate output\n",
        "        sample_text = \"O Romeo, wherefore art thou\"\n",
        "        inp = train_dataset.char_tensor(sample_text)\n",
        "        sample_input = Variable(inp).cuda().unsqueeze(0).float()\n",
        "        out_test = topk_sampling_lstm(model,sample_input, 300)[0]\n",
        "        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n",
        "        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "        print(\"Top-K sampling -----------------\")\n",
        "        print(out_chars)\n",
        "\n",
        "        out_test = greedy_sampling_lstm(model,sample_input, 300)[0]\n",
        "        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n",
        "        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "        print(\"Greedy sampling ----------------\")\n",
        "        print(out_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# output checkpoint\n",
        "torch.save(model.state_dict(), LSTM_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwQEM1JHzDo-"
      },
      "source": [
        "# Task 2: Character generation transformer network implementation\n",
        "Our simple transformer-like network will take as input a sequence of characters and predict the next character in the sequence. To ensure an efficient training procedure, masked attention modules will be used as in the [GPT model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
        "\n",
        "For this task you must implement the Scaled dot product attention module and the Masked multi-head attention module. Both of these modules are described in the [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper (See Figure 2 in the paper as well as Sections 3.2.1, 3.2.2 and 3.2.3). They are the core operations of transformers. As we will use our model for text generation also add the masking operation shown as (mask opt.) in Figure 2, implemented as AttentionMasking in the code.\n",
        "\n",
        "**Implement the modules in the ScaledDotProductAttention class and the MultiHeadAttention class.**\n",
        "\n",
        "Read the GPT paper and the Attention is all you need paper for a better understanding of the components. For a more high level overview, this [post](https://jalammar.github.io/illustrated-gpt2/) may also be helpful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "lNhqnTm8zGRe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "        # Positional encoding adds the positional information to the\n",
        "        # embedding. Without it the model would not be able to differentiate\n",
        "        # between different characters orders such as between \"dog\" and \"god\".\n",
        "        position = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div_term = 10000.0**(torch.arange(0,d_model,2).float()/d_model)\n",
        "        print(div_term.shape)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position / div_term)\n",
        "        pe[:, 1::2] = torch.cos(position / div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.pe = pe.cuda()\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.pe[:, :x.size(1)]\n",
        "        return p\n",
        "\n",
        "class AttentionMasking(nn.Module):\n",
        "    def __init__(self, max_len):\n",
        "        super(AttentionMasking, self).__init__()\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_len, max_len))\n",
        "                                     .view(1, 1, max_len, max_len))\n",
        "    def forward(self,x):\n",
        "        length = x.shape[-1]\n",
        "        out = x.masked_fill(self.mask[:,:,:length,:length] == 0, float('-inf'))\n",
        "        return out\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, max_len):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.mask_opt = AttentionMasking(max_len)\n",
        "        self.scale = 1.0 /  math.sqrt(max_len)\n",
        "\n",
        "\n",
        "    def forward(self,q,k,v):\n",
        "        # Implement the scaled dot product attention as described in\n",
        "        # the Attention is all you need paper in Equation 1\n",
        "        \n",
        "        attention = q @ k.transpose(-1, -2) * self.scale\n",
        "        attention = attention + self.mask_opt(attention)\n",
        "        \n",
        "        attention_weights = self.softmax(attention)\n",
        "        \n",
        "        outputs = attention_weights @ v\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim_model, num_neuron, n_head, max_len):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.num_neuron = num_neuron\n",
        "\n",
        "        # self implemented attention\n",
        "        self.attention = ScaledDotProductAttention(max_len)\n",
        "\n",
        "        # liner tranforms in the begining of the block\n",
        "        self.q_transform = nn.Linear(dim_model, n_head * num_neuron)\n",
        "        self.k_transform = nn.Linear(dim_model, n_head * num_neuron)\n",
        "        self.v_transform = nn.Linear(dim_model, n_head * num_neuron)\n",
        "\n",
        "        # output linear transforms\n",
        "        self.out_linear = nn.Linear(n_head * num_neuron, dim_model)\n",
        "        \n",
        "\n",
        "    def split(self,tensor):\n",
        "        batch_size, length, total_dim = tensor.size()\n",
        "        # Reshape the tensor to enable the use in\n",
        "        # the ScaledDotProductAttention module\n",
        "        split_tensor = tensor.view(batch_size, length, self.n_head, self.num_neuron).transpose(1,2)\n",
        "        return split_tensor\n",
        "\n",
        "    def concat(self,tensor):\n",
        "        batch_size, num_heads, length, num_neuron = tensor.size()\n",
        "        # Reshape the tensor to its original size before the split operation.\n",
        "        concat_tensor = tensor.transpose(1,2).contiguous().view(batch_size, length, self.n_head*self.num_neuron)\n",
        "        return concat_tensor\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # Apply linear layer to make them fit the corect size\n",
        "        q_trans = self.q_transform(q)\n",
        "        k_trans = self.k_transform(k)\n",
        "        v_trans = self.v_transform(v)\n",
        "        \n",
        "        # Split into multiple heads with the provided function\n",
        "        q_split = self.split(q_trans)\n",
        "        k_split = self.split(k_trans)\n",
        "        v_split = self.split(v_trans)\n",
        "        \n",
        "        # Process attention and merge them back\n",
        "        out = self.concat(\n",
        "            self.attention(q_split, k_split, v_split)\n",
        "        )\n",
        "\n",
        "        return self.out_linear(out)\n",
        "\n",
        "class PositionFeedForwardNet(nn.Module):\n",
        "    def __init__(self, dim_model):\n",
        "        super(PositionFeedForwardNet, self).__init__()\n",
        "        self.ff_net1 = nn.Linear(dim_model, dim_model*4)\n",
        "        self.ff_net2 = nn.Linear(dim_model*4, dim_model)\n",
        "    def forward(self,x):\n",
        "        ff_out = self.ff_net1(x)\n",
        "        ff_out = torch.nn.functional.relu(ff_out)\n",
        "        ff_out = self.ff_net2(ff_out)\n",
        "        return ff_out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim_model, num_neuron, n_head, max_len):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(dim_model, num_neuron, n_head, max_len)\n",
        "        self.l_norm = torch.nn.LayerNorm(dim_model)\n",
        "        self.l_norm2 = torch.nn.LayerNorm(dim_model)\n",
        "        self.ff_net = PositionFeedForwardNet(dim_model)\n",
        "        # b, len_seq, n_head, num_neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "      # A Transformer block as described in the\n",
        "      # Attention is all you need paper. In Figure 1 the transformer\n",
        "      # block is marked with a gray rectangle right of the text \"Nx\"\n",
        "      _x = x\n",
        "      mha1 = self.mha(x,x,x)\n",
        "      lnorm = self.l_norm(_x+mha1)\n",
        "      _x = lnorm\n",
        "      ff_out = self.ff_net(lnorm)\n",
        "      out = self.l_norm2(ff_out+_x)\n",
        "\n",
        "      return out\n",
        "\n",
        "class TransformerSimple(nn.Module):\n",
        "    def __init__(self, seq_length, input_dim, output_dim,\n",
        "                 batch_size):\n",
        "        super(TransformerSimple, self).__init__()\n",
        "        num_neuron = 64\n",
        "        n_head = 8\n",
        "        dim_model=256\n",
        "        max_len = 512\n",
        "        self.start_embedding = nn.Embedding(input_dim, dim_model)\n",
        "\n",
        "        self.pos_embedding = PositionalEncoding(dim_model)\n",
        "\n",
        "        # b x l x c*n_head\n",
        "        self.t_block1 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block2 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block3 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block4 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block5 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "\n",
        "        #self.out_layer_1 = nn.Linear(dim_model, dim_model)\n",
        "        self.output_layer = nn.Linear(dim_model,output_dim)\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "      # x - Tensor - (b, seq_len)\n",
        "      # Embeds the input tensor from tokens to features\n",
        "      s_emb = self.start_embedding(x)\n",
        "      # Adds positional embeddings\n",
        "      p_emb = self.pos_embedding(s_emb)\n",
        "      b_out = p_emb + s_emb\n",
        "      # Transformer blocks - You can experiment with varying depth\n",
        "      # For example GPT uses 12 blocks but this might be a bit memory intensive\n",
        "      b_out = self.t_block1(b_out)\n",
        "      b_out = self.t_block2(b_out)\n",
        "      b_out = self.t_block3(b_out)\n",
        "      b_out = self.t_block4(b_out)\n",
        "      b_out = self.t_block5(b_out)\n",
        "\n",
        "      # Output mapping to a classification of output tokens\n",
        "      # For each token the network tries to predict the next token\n",
        "      # based only on the previous tokens.\n",
        "      # Output shape: (b x seq_len x vocabulary_size)\n",
        "      out = self.output_layer(b_out)\n",
        "\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPjl6ttzPHsq"
      },
      "source": [
        "## Dataset class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "T04eYePr8Gn3"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, chunk_len=200, padded_chunks=False):\n",
        "        # Character based dataset\n",
        "        dataset_path = \"./input.txt\"\n",
        "        # The tokens in the vocabulary (all_characters)\n",
        "        # are just the printable characters of the string class\n",
        "        self.all_characters = string.printable\n",
        "        self.n_characters = len(self.all_characters)\n",
        "        # Maps characters to indices\n",
        "        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n",
        "        self.file, self.file_len = self.read_file(dataset_path)\n",
        "        # Sequence length of the input\n",
        "        self.chunk_len = chunk_len\n",
        "        self.encoded_file = [self.char_dict[x] for x in self.file]\n",
        "\n",
        "    def read_file(self,filename):\n",
        "        file = unidecode.unidecode(open(filename).read())\n",
        "        return file, len(file)\n",
        "\n",
        "    def encode_text(self,in_str):\n",
        "        # in_str - input sequence - String\n",
        "        # Returns - in_str mapped to tokens in char_dict\n",
        "        tensor = torch.LongTensor([self.char_dict[x] for x in in_str])\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target = self.get_random_text()\n",
        "        return {\"input\":inp, \"target\":target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def get_random_text(self):\n",
        "        # Pick a random string of length self.chunk_len from the dataset\n",
        "        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n",
        "        end_index = start_index + self.chunk_len + 1\n",
        "        chunk = self.encoded_file[start_index:end_index]\n",
        "        # input_tokens - random sequence of tokens from the dataset\n",
        "        input_tokens = torch.LongTensor(chunk[:-1])\n",
        "        # target - input token sequence shifted by 1\n",
        "        # the idea is to predict next token for each token in the input sequence\n",
        "        # therefore if the input is [1,2,3,4] the target is [2,3,4,5]\n",
        "        target = torch.LongTensor(chunk[1:])\n",
        "        input_tokens = input_tokens.cuda()\n",
        "        target = target.cuda()\n",
        "        return input_tokens, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhvayVAjsCMf"
      },
      "source": [
        "## Character sampling\n",
        "\n",
        "To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n",
        "\n",
        "In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n",
        "\n",
        "In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "3IVliOUqqEd5"
      },
      "outputs": [],
      "source": [
        "def topk_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n",
        "    # x -- b x onehot_char\n",
        "    # x = b x l\n",
        "    outputs = torch.zeros((1,num_chars))\n",
        "    inp = x\n",
        "\n",
        "    for t in range(num_chars):\n",
        "        # b x onehot_char\n",
        "        output = model(inp.long())[0,-1:]\n",
        "        #output = torch.softmax(output, dim=1)\n",
        "        # b x 3\n",
        "        output_vals, output_ind = torch.topk(output, 5, dim=1)\n",
        "        # 3 -> int\n",
        "        output_vals = torch.softmax(output_vals, dim=1)\n",
        "        top_ind = torch.multinomial(output_vals[0], 1)[0]\n",
        "        # int\n",
        "        out_char_index = output_ind[0,top_ind]\n",
        "        # int -> 1\n",
        "        out_char_index = torch.ones(1).cuda() * out_char_index\n",
        "\n",
        "        outputs[:,t] = out_char_index.item()\n",
        "        if inp.shape[1] > chunk_len:\n",
        "          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n",
        "        else:\n",
        "          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def greedy_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n",
        "    # x -- shape (batch, tokens in x)\n",
        "    outputs = torch.zeros((1,num_chars))\n",
        "    inp = x\n",
        "\n",
        "    for t in range(num_chars):\n",
        "        # b x l x onehot_char\n",
        "        output = model(inp.long())[0,-1:]\n",
        "        output = torch.softmax(output, dim=1)\n",
        "        out_char_index = torch.argmax(output, dim=1)\n",
        "        outputs[:,t] = out_char_index.item()\n",
        "        if inp.shape[1] > chunk_len:\n",
        "          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n",
        "        else:\n",
        "          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6X7-Tlc2iqh"
      },
      "source": [
        "## Transformer model training\n",
        "\n",
        "With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n",
        "especially with the sequence length (chunk_len) used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "gY8aZz1R2g3M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 788.91chunks/s, loss=2.56, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on  me s thit mere m s t tho s mind se thes hange we at ate te thin she my me that me se al ar t s my arorind we miche and,\n",
            "\n",
            "\n",
            "Ant mathathe s ather touthear man me wathillerd alanghour thar tho tind s t stishe she s she t mant w s t s my wind tin an mathit s the thirs an meris s my shan mathou m t my, thour sthar tin my shanondeangr astheand, an s m sto m manderoreathe m m arenghat s al ser angh whe m\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "\n",
            "I meshe me thasthathingout s w t wher sthise t me wh se te at as windsthas ar t t thale me me wice wirithas mand t then ar h theantheshand antind s ticoule s al st t me aser me my me st ateren t tith w my t m st masthe thingen t thang ale ast win se there thal mind thin t me weroton my meast wingous me s me thato alle me mear th my mash thotis m athiles w an whit t s we atind anor thas s mard th\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 0/10: 10240chunks [00:18, 552.91chunks/s, loss=2.56, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament,  te t s mit marererd theard at wash allllerind ateng wichan m thiche athise myo tis she wis sh s the thot m se t th he st s t m andinds se as an wat s wear s weritous hous whour wil sise t s manthashanouris my t wind s t an me angrenge ase thill me shean s wire m mar the an t ate t we alisheanth withit t my, at t w t w win theatheat weser tharishe wis sear mer and as sh wealesease we are an tharer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 1/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 803.01chunks/s, loss=2.41, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on  t ho tithe thild s s se the ate t ore on as mere thend thir hesther thin thor the me t mas t soullle antom al ass athilllir s t boul alll seathotong bust thor me sthas t s sthes tertin tiste bareands t s thin t s asthe seanes ar te athaten tistound t sthile thenting mase t mat the mind t tous stheate t m sen toull te buner as t ase at ar ane tengor thor theant bend athesthe s t alle tin be bre be\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "And alou to he st mingo thow mate s thes,\n",
            "I alerd, her thin hand bund ald myore me hond be tous bo atho s angr misther tomyon ar bu stounde t sse alon men me stho t be bestis be ther tom t sthes be se s mango and silis m tent blllin bre to se ters bl ates t thoth miler ber s an berst stondind me tour sthe an s thathe thotire thatothingr mane t bere stis s\n",
            "Anengharert s se s thes test man sthesend\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 1/10: 10240chunks [00:18, 553.27chunks/s, loss=2.41, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament,  our tond totit t anors,\n",
            "Thist t his s maliril ato to s t malir thang thast athit at at thit and,\n",
            "The tir t atothande br s st be t ano te s s sthorers be arere ss serire the tors an to tis my\n",
            "Allind thalang as, ass br bate anthas,\n",
            "Thare stis meres, as my.\n",
            "Malend,\n",
            "\n",
            "My,\n",
            "\n",
            "Whithit t my s al by buthe s te at stissthing t mound t at mo tor ath moutes tomouthanou t tomend ar athir t sthasst my\n",
            "Ard me te \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 2/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 785.22chunks/s, loss=2.25, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on  and has there thall he\n",
            "I har hean the the marer s alore wins\n",
            "An the s alome she mas sthind m tin hen alis,\n",
            "s sthallll titime ts mange ss areates s maro to s andis, the ss thereeres,\n",
            "An heat he t hiromat, meng t s tharin s sheas merere at merare s heseange st heares s\n",
            "Thasen me my thitheas athe t as s has han ssss hindares here matile me s tistis ande t mange ands\n",
            "Thitere s, therat he s wind shate\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end:  han wimar then\n",
            "Trir se thus thasth te hangre hous,\n",
            "Anche whe s himen heealll angst hat sstis, t areand, te atimitis ang, tes h witime s\n",
            "Angheand t thalale, me thouse s, hent healis we se as s hanengan tere s\n",
            "s anote therar he marere m atomendes an at hear s thy sheeeround heas m t mitin hisend, tes, athe henthes,\n",
            "Thean t henounghe ss has mirinds mar mas thanghe, ssean te as an thistes my marith t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 2/10: 10240chunks [00:18, 549.35chunks/s, loss=2.25, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament, \n",
            "To the hast the thass s andeangenger ath ar wo tores ss hale s tomaran ar the than sterangh man he to my mishin th thithe this\n",
            "Ther mathe se he shende thear me te here hinthareng s as thes aller ss thes thares s s, tatiserest\n",
            "The t thit theat mas tharist at hasssest t s shon teanthe s,\n",
            "Thas s wofeat alle the man tinghe s an are ales s meather merongere aresterte ar se athearer s seatere st\n",
            "s andi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 3/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 786.61chunks/s, loss=2.12, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on enth and\n",
            "tourd sist hathis tound, wis toundesen\n",
            "This to henow arith he tof my hours aves tande theall\n",
            "Whave hise ast ofive an beresisesedstes teneed\n",
            "Thimous ouge ancheneser t oreeale thyof henener broust\n",
            "Ant hind therar andis haver t seis ancard ous then arise\n",
            "Ind, ane aneses o thof ha anour hanes t bea o ondeed\n",
            "I thaner,\n",
            "Andis thive and an tashis hean athe bleerent teease\n",
            "\n",
            "Then asend anonened ato\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "I's mat than to she ave soulld sownt our han sim house\n",
            "The hat seange t anere anen henomenged, tousstht tathat\n",
            "Tan buse andound oustont aver hyowest heree tee ond anoullle\n",
            "The oune ount onowst then beartessse aris t thasees t\n",
            "Wit heanderd hive aves,\n",
            "Henghy speatye omere avind, ben beset ondsthenen\n",
            "Wese t sispeangsty anoued t oun thind heaneds of theeas\n",
            "Thesser bene tiste aras tono toulld, then he\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 3/10: 10240chunks [00:18, 550.26chunks/s, loss=2.12, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament, \n",
            "Thy terare his hingne tated steees oull hit.\n",
            "\n",
            "Thas an t hyourest asp thanded, heit ath bee arer beaste heand.\n",
            "I anand I hanough t thoouer hthan t heist,\n",
            "Isthen t tinoule be bratowes and tenous t tashiss ouses\n",
            "Whousted t sono himeer hist ate teee tyoullds.\n",
            "\n",
            "Andis seast ano hese and heanes, hand as spave beave\n",
            "Issth oueend heand ave hee tinde outous and henendes\n",
            "Ise as beard ome and ounes an beaned\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 4/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 794.16chunks/s, loss=2.04, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on tred, and hor as whavengind and,\n",
            "To min histe the sen mary my hereser ontand weald a thase where\n",
            "And heand the him touse hthaves omoun onchend,\n",
            "Wis an hore owase tt heenger oneansthios owar thast.\n",
            "\n",
            "\n",
            "CLAUCELIUS:\n",
            "And my hispessesen ta ofee hise as myon a heant.\n",
            "An Reass my anousent his splean an toupse teand\n",
            "A o myshe my hise hye trean ame hindseng trinces\n",
            "I sthentes hourd ano of him thissess.\n",
            "This \n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "Thist theard ay severences hee areass offortheres.\n",
            "\n",
            "PERIO:\n",
            "Thavers to hea m anof hyond sheave ad a shan ands\n",
            "Aspe ontr theay ass andserd and anoter and a thises\n",
            "s hea tea he thee as tra thime omppe send ifracesss\n",
            "Tofore ano indithis are o theasest.\n",
            "\n",
            "An ELICHABET:\n",
            "Ay whor hyou avengnce tooman t hofee her thofe\n",
            "I him sthee tere t o hear the ofee sof hofour,\n",
            "An hyone avasht han heand shy the team t \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 4/10: 10240chunks [00:18, 552.59chunks/s, loss=2.04, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament, \n",
            "In and the herees and there hear tinges and\n",
            "Ast hind theass herar and thendes hand amin owend asthere\n",
            "Wo he hast asear te ofend trend tofee arrrthe toue hear\n",
            "Tomenes ane andy ofoned and honomer tasps an toone,\n",
            "A han wim hy shin own ane theate own hent harid ome\n",
            "And sthen trimenes on hea ast hand he and ond ares\n",
            "And weand the a hend anen one hean here heave,\n",
            "And winde hise ass a hand therer teas h\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 5/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 779.96chunks/s, loss=1.88, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on er ontard,\n",
            "As well to my haverthouns, will and sperss,\n",
            "But the shee strest themst ander'd ther wis my drecot.\n",
            "\n",
            "\n",
            "ClORUMNIO:\n",
            "\n",
            "My whest word art,\n",
            "I malllty truesty, annd toursessts, o stof willitstst\n",
            "I trevestt ave a themppe on to tof splo trisest\n",
            "Who the the that at ande aswnd allo te heatst\n",
            "Wis that at ournchts oforrd, werrd heldsst tinde.\n",
            "Wit My, trerry thou herrd atherts orer hand.\n",
            "Whowndsh shall\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "Thaven, to warder all the ther my serrdest,\n",
            "Whill word he hom wnentst ofo thas ourt,\n",
            "Buntt are won withelld ouch a a alll tontsssend feertth.\n",
            "\n",
            "What Clisttt, ay mond sheat theence wend\n",
            "To mance otatte in tound, the wit havencht the imard,\n",
            "Whe his ade t soulld thanckst at o wontht wa thith\n",
            "The sheaved d sprestst oundsir of tof tree t\n",
            "Who hy sthent ourth theer at, ta toffte to whee\n",
            "Whertst were toff\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 5/10: 10240chunks [00:18, 548.88chunks/s, loss=1.88, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament, \n",
            "As ward wand my the so deved arnd word this at\n",
            "that t hyo herld an twellt o heastst are to this\n",
            "Then peat it thengh tree tthath tho and tallly a a a dond.\n",
            "A Stharrvou dof ar ane as thist and weastct trestst,\n",
            "Withtch wis thingnght thisst trit o spevichst\n",
            "Whath arid wand wandlly by thee t sha oulllt and,\n",
            "I wear that serrtt ifoong honghird,\n",
            "Whit, thought tinde a an and terrt windlst,\n",
            "I my worrd ashi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 6/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 782.10chunks/s, loss=1.81, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on  ans bright,\n",
            "As bure sone a with him hone off meent of mye.\n",
            "\n",
            "KING RICHARD III:\n",
            "Ay me sight the sake would has wand them aterre and thee\n",
            "A spoffftires on and a wa whis one of hear,\n",
            "Whath spare tandy ale tharm he arm on ast teall and\n",
            "Off wit of at a mastcend your toundse, aye beash'd,\n",
            "Stird alo of an that and a whan bearnce ound.\n",
            "\n",
            "\n",
            "KING RICHARD:\n",
            "HIIf h mam he seam orme wande thit tooo ommme.\n",
            "\n",
            "\n",
            "DUCHO\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "The wort wall a with tho well woul a be so winded.\n",
            "And thy sore wende, are it ande ond thele torld\n",
            "And amally ont amirtty allid,\n",
            "On thar as thame wend ofe would weay as heasse\n",
            "I wit ore wisth ounghtiong on thame orreanth,\n",
            "Thereint off the in tomee,--beancht and win hath at tremante, as wouch's and the\n",
            "Whe that heave offf and thime is tond outht an\n",
            "theme oforthe they was witheres oreanth wit ofthi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 6/10: 10240chunks [00:18, 551.24chunks/s, loss=1.81, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament, \n",
            "We and the whan she sporeast and a brown\n",
            "As should tree att arert thonk an weante ist,\n",
            "Inde tomee think of thee our ar toff theem,\n",
            "Ang wit as were as a the and ofell on oust.\n",
            "There hase ave off hince theat, ore and and thome\n",
            "As he the shee weas ore aspireniciarions of head,\n",
            "That I stree ivow an a as a whybeare armace ount,\n",
            "Whit wit are on tof take amirt ore amppost\n",
            "Of ther hane our an our thasst \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 7/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 799.49chunks/s, loss=1.7, lr=0.0006, run:=Transformer] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on  and to sell.\n",
            "Botcke you: there steervet me shall, attered,\n",
            "And to me sears. O a sare, to this the hearth,\n",
            "And ware the tought are and the are of hof theest,\n",
            "And the toure though has bow oran and and,\n",
            "And be the are ofther ache areasts it heared,\n",
            "Orether wince hims treand, ond an beatther,\n",
            "I seaven, or and theathen, thoushe he head,\n",
            "Of with wife to ale ove ore other our hand,\n",
            "And seay tallle to an\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "Who tell that wartedere to said, was warent to my son.\n",
            "Then ane welll so to the an the sare, and thows here\n",
            "Tould tongumes, and be thowse illes wead,\n",
            "In wan we has andst wheat, and the wering hasst.\n",
            "Whond an woult, then, I ast and word at alay ond this\n",
            "Tones all the and on steeed to shand and tand the telll\n",
            "Ouse to and so heaved hinghe if an worth his wonest.\n",
            "Arot:\n",
            "I welll, wear in windomer are o\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 7/10: 10240chunks [00:18, 553.25chunks/s, loss=1.7, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament, \n",
            "And here thangets of hanguereds, sorret heart.\n",
            "I woulll weare, and, at I thoust hand ofing,\n",
            "That thims soraninst arught, oun,\n",
            "This at bloookes ane tofend theares of thee.\n",
            "\n",
            "\n",
            "BOLINDEO:\n",
            "Wetweren at if anccame the and,\n",
            "Wand the weare how sarrre owes shille awe sto\n",
            "Tond selling sealings, and toue therencand tates,\n",
            "Werre the are ign torath at herllieds and and\n",
            "As and are thim s shayst fore, at warere,\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 8/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 711.29chunks/s, loss=1.55, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on  her speass folk my shall mother,\n",
            "Where may statther both seat me strans\n",
            "Thy be sensen the for shalt my son or on thim\n",
            "Were hor onour athe andverting them our tare.\n",
            "And das speed on of sherms,\n",
            "O sor and moves ore helpser are ims alll as as\n",
            "ddarrd im, the toumb dofth and the opes,\n",
            "O of the thouse shire and his onste of of shigld.\n",
            "\n",
            "\n",
            "BRAMPSTEY:\n",
            "Whas dredged.\n",
            "\n",
            "\n",
            "\n",
            "MAUCAREN:\n",
            "Itheren, is this this denerat\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "Which thour stand mother shir his stand all for\n",
            "The chird toons the oward thy ound spent on the\n",
            "Tourn sonstran o hither.'s I weard as as onde wofes.\n",
            "Thave and on andird thence ore spirs.\n",
            "The desep irems as as offfffircerss\n",
            "To mene at our of heasps your ourse.\n",
            "\n",
            "\n",
            "GRUMIO:\n",
            "Nay, meay mead is the them, forsh, treminds,\n",
            "Is denem othing of the thare.\n",
            "\n",
            "\n",
            "PENETER:\n",
            "\n",
            "Weshy has denombed are of athishen ass a t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 8/10: 10240chunks [00:18, 544.98chunks/s, loss=1.55, lr=0.0006, run:=Transformer]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament, \n",
            "There staing them many the sonss a my shar.\n",
            "\n",
            "BRUCKINGHANG:\n",
            "Welll, so mee spropist of mis theme on tof oust,\n",
            "The and ormes aragand as and as a a devidin\n",
            "I the out dong one atrmisted.\n",
            "\n",
            "\n",
            "BRUCKENGHARORD:\n",
            "As I darrm, me as a off them.\n",
            "\n",
            "\n",
            "Prevent:\n",
            "Where and arm as offftaicess.\n",
            "\n",
            "\n",
            "PROMPERO:\n",
            "\n",
            "MEddgham, this\n",
            "\n",
            "Wheren:\n",
            "Weld marrd to to men at a ass thygarrl and\n",
            "Is say to ammpps then is teld.\n",
            "\n",
            "\n",
            "DUCHESSSSS:\n",
            "Ay,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 9/10: 100%|█████████▉| 9984/10000 [00:12<00:00, 780.70chunks/s, loss=1.56, lr=0.0006, run:=Transformer]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-K sampling\n",
            "----------------------------------------\n",
            "What authority surfeits on \n",
            "That that her but and the power thy, both stand.\n",
            "\n",
            "KING RICHARD III:\n",
            "Ay, sir? his those!\n",
            "\n",
            "MERCUTIUS:\n",
            "My darrre toff thy your\n",
            "The sowe our of or acclliaie.\n",
            "\n",
            "\n",
            "MESCUMIUS:\n",
            "Nay am send and thee terell impond,\n",
            "I would womb an our of our tresth,\n",
            "My anny wish our athen his are the bouth\n",
            "Are wencesss, wis an my trelfiaght.\n",
            "\n",
            "\n",
            "Kerspeat:\n",
            "Here thare ond tingle:\n",
            "I am wish our are our hear at thouse then,\n",
            "Our se\n",
            "----------------------------------------\n",
            "I say unto you, what he hath done famously, he did it to that end: \n",
            "I thank his me show hom his her hurse?\n",
            "\n",
            "First My Moscenaly:\n",
            "And, werat if atwirst thim angreroan thim.\n",
            "\n",
            "\n",
            "MARARGHEN:\n",
            "Welll, I mearr, the I have I wom woesh:\n",
            "My hare iss freather a wourd with as though.\n",
            "\n",
            "\n",
            "PEOMNEY:\n",
            "Were thown:\n",
            "Whis thate are in thouse bacck at in itre.\n",
            "\n",
            "\n",
            "MORCUSTIO:\n",
            "Wenthing ano man ore of acccissaly?\n",
            "\n",
            "MERCUMIO:\n",
            "My dourtur once torrme, and word I'lll again.\n",
            "\n",
            "\n",
            "KING RICHARD:\n",
            "What I wer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training - Epoch: 9/10: 10240chunks [00:18, 542.71chunks/s, loss=1.56, lr=0.0006, run:=Transformer]                      "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "That in submission will return to us: And then, as we have ta'en the sacrament, \n",
            "To so murder of a comes man my breach,\n",
            "Where a slive an ass ime.\n",
            "\n",
            "\n",
            "MENENENIUS:\n",
            "I am then it tof and that and.\n",
            "\n",
            "\n",
            "KING RWAMP:\n",
            "Witch, is ir myow lord merrrie it.\n",
            "\n",
            "\n",
            "KIRGHAM:\n",
            "Misshaged is merrce it the and in tolly.\n",
            "\n",
            "\n",
            "KINGHARD IIII:\n",
            "No welll I, heav I well I am and agrry.\n",
            "\n",
            "\n",
            "KARINGHARD:\n",
            "And wenre tire:\n",
            "Then ange thare I tount thim sence ome\n",
            "And senatiur an werre there.\n",
            "\n",
            "\n",
            "GLOUCOMESTE:\n",
            "Near, there ance o\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#Sample parameters, use whatever you see fit.\n",
        "batch_size = 256\n",
        "chunk_len = 128\n",
        "train_dataset = TextDataset(chunk_len=chunk_len)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "input_dim = train_dataset.n_characters\n",
        "output_dim = train_dataset.n_characters\n",
        "learning_rate = 0.0006\n",
        "\n",
        "model = TransformerSimple(chunk_len, input_dim, output_dim,batch_size)\n",
        "model.train()\n",
        "model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs=10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # inputs - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n",
        "            inputs = data['input'].long()\n",
        "            # labels - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n",
        "            labels = data['target'].long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            target_t = labels\n",
        "            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target_t.view(labels.shape[0]*labels.shape[1]))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            prog_bar.set_postfix(**{'run:': \"Transformer\", 'lr': learning_rate,\n",
        "                                    'loss': loss.item()\n",
        "                                    })\n",
        "            prog_bar.update(batch_size)\n",
        "\n",
        "        # Intermediate text output\n",
        "        sample_texts = [\"What authority surfeits on\",\n",
        "                        \"I say unto you, what he hath done famously, he did it to that end:\",\n",
        "                        \"That in submission will return to us: And then, as we have ta'en the sacrament,\"]\n",
        "        output_token = torch.zeros(1,1).cuda()\n",
        "        output_token[0,0] = train_dataset.n_characters-1\n",
        "        print(\"Top-K sampling\")\n",
        "        for sample_text in sample_texts:\n",
        "            sample_encoding = train_dataset.encode_text(sample_text)\n",
        "            sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "\n",
        "            #out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "            out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "            out_char_index = out_test.long().detach().cpu().numpy()\n",
        "            out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "            print(\"----------------------------------------\")\n",
        "            print(out_chars)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-Tyjm1kHz5Z"
      },
      "source": [
        "## Text sampling - Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "XjvbjEdjH36Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "To be or not to be? \n",
            "\n",
            "MENENIUS:\n",
            "I will not the so the so the so the some.\n",
            "\n",
            "MENENIUS:\n",
            "I will the some of the the some of the some\n",
            "To then are of the our an the our are of thee\n",
            "The wear our an the our are of the our are\n",
            "The our off our the our an the our are of thee\n",
            "And the and the our an the our are of thee\n",
            "And the and the our an the our are of thee\n",
            "And the and the our an the our are of thee\n",
            "And the and the our an the\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"To be or not to be?\"\n",
        "sample_encoding = train_dataset.encode_text(sample_text)\n",
        "sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "\n",
        "out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "# out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "out_char_index = out_test.long().detach().cpu().numpy()\n",
        "out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "print(\"----------------------------------------\")\n",
        "print(out_chars)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
