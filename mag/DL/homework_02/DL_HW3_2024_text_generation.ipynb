{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2e0GYiqPjk"
      },
      "source": [
        "# Homework 3 - Text generation with LSTM and Transformer networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_hn9dHcPKt4"
      },
      "source": [
        "## Installs the unidecode library and downloads the Shakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qpdcKW580xG"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UG5SIDaHKXC"
      },
      "source": [
        "## LSTM implementation\n",
        "\n",
        "For this task you will implement the LSTM neural network architecture and train it on the task of character-level text generation. Implement a single layer LSTM and optionally extend your implementation to multiple layers to generate better results.\n",
        "\n",
        "Links:\n",
        "\n",
        "- https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html -- Lists the equations for each component of the LSTM cell.\n",
        "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/ -- Intuitive explanation of LSTM\n",
        "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/ -- Explanation and uses of RNNs.\n",
        "\n",
        "\n",
        "Implement the initialization and the forward pass of a LSTMCell and use it as part of the LSTMSimple network class.\n",
        "\n",
        "The input of the LSTM network will be a sequence of characters, whereas the input of the LSTMCell will be a single input character (x), the output of the previous iteration (C) and the hidden state of the previous iteration (h). Iteratively process the entire input character sequence and calculate the loss based on the prediction at each time step.\n",
        "\n",
        "### Do NOT use the torch.nn.LSTM class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma-reelTqEJ7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "\n",
        "        super(LSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        ## TODO: Initialize the necessary components\n",
        "\n",
        "    def forward(self, x, C, h):\n",
        "        # x - batch of encoded characters\n",
        "        # C - Cell state of the previous iteration\n",
        "        # h - Hidden state of the previous iteration\n",
        "\n",
        "        # Returns: cell state C_out and the hidden state h_out\n",
        "\n",
        "        #TODO: implement the forward pass of the LSTM cell\n",
        "        pass\n",
        "\n",
        "class LSTMSimple(nn.Module):\n",
        "    def __init__(self, seq_length, input_dim, hidden_dim, output_dim,\n",
        "                 batch_size):\n",
        "        super(LSTMSimple, self).__init__()\n",
        "\n",
        "        self.seq_length = seq_length\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        ## TODO: Initialize the LSTM Cell and other potential necessary components\n",
        "        # You can use a nn.Linear layer to project the output of the LSTMCell to\n",
        "        # self.output_dim.\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x - One hot encoded batch - Shape: (batch, seq_len, onehot_char)\n",
        "\n",
        "        # Returns the predicted next character for each character in the\n",
        "        # sequence (outputs), also returns the cell state and hidden state of the\n",
        "        # LSTMCell call on the last character. -- outputs, (c,t)\n",
        "\n",
        "        #TODO: Implement the forward pass over the sequenece of characters\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A1_UtJrKnU-"
      },
      "source": [
        "### LSTM Sampling Code\n",
        "\n",
        "To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n",
        "\n",
        "In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n",
        "\n",
        "In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X23_lg53Kqj2"
      },
      "outputs": [],
      "source": [
        "def greedy_sampling_lstm(lstm, x, num_chars):\n",
        "    # x -- b x onehot_char\n",
        "    outputs = torch.zeros((1,num_chars,x.shape[2]))\n",
        "    t_outputs, (cell_state, hidden) = lstm(x.float())\n",
        "    for c in range(num_chars):\n",
        "        output_tmp = torch.softmax(lstm.proj(hidden),dim=1)\n",
        "        top_ind = torch.argmax(output_tmp,dim=1)[0]\n",
        "        tmp = torch.zeros_like(x[:,0,:]).cuda()\n",
        "        tmp[:,top_ind] = 1\n",
        "        outputs[:,c] = tmp\n",
        "\n",
        "        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n",
        "    return outputs\n",
        "\n",
        "def topk_sampling_lstm(lstm, x, num_chars):\n",
        "    # x -- b x onehot_char\n",
        "    outputs = torch.zeros((1,num_chars,x.shape[2]))\n",
        "    t_outputs, (cell_state, hidden) = lstm(x.float())\n",
        "    for c in range(num_chars):\n",
        "        output_vals, output_ind = torch.topk(lstm.proj(hidden), 5, dim=1)\n",
        "        output_tmp = torch.softmax(output_vals,dim=1)\n",
        "        top_ind = torch.multinomial(output_tmp[0], 1)[0]\n",
        "        tmp = torch.zeros_like(x[:,0,:]).cuda()\n",
        "        tmp[:,output_ind[0,top_ind]] = 1\n",
        "        outputs[:,c] = tmp\n",
        "\n",
        "        cell_state, hidden = lstm.lstm_cell(tmp,cell_state,hidden)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJdxAOVsKzBX"
      },
      "source": [
        "### LSTM Dataset Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5U4jzUDK1dG"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class LSTMDataset(Dataset):\n",
        "    def __init__(self, chunk_len=200, padded_chunks=False):\n",
        "        # Character based dataset\n",
        "        dataset_path = \"./input.txt\"\n",
        "        # The tokens in the vocabulary (all_characters)\n",
        "        # are just the printable characters of the string class\n",
        "        self.all_characters = string.printable\n",
        "        self.n_characters = len(self.all_characters)\n",
        "        # Maps characters to indices\n",
        "        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n",
        "        self.file, self.file_len = self.read_file(dataset_path)\n",
        "        # Sequence length of the input\n",
        "        self.chunk_len = chunk_len\n",
        "\n",
        "    def read_file(self,filename):\n",
        "        file = unidecode.unidecode(open(filename).read())\n",
        "        return file, len(file)\n",
        "\n",
        "    def char_tensor(self,in_str):\n",
        "        # in_str - input sequence - String\n",
        "        # Return one-hot encoded characters of in_str\n",
        "        tensor = torch.zeros(len(in_str),self.n_characters).long()\n",
        "        char_ind = [self.char_dict[c] for c in in_str]\n",
        "        tensor[torch.arange(tensor.shape[0]),char_ind] = 1\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target = self.get_random_text()\n",
        "        return {\"input\":inp, \"target\":target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def get_random_text(self):\n",
        "        # Pick a random string of length self.chunk_len from the dataset\n",
        "        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n",
        "        end_index = start_index + self.chunk_len + 1\n",
        "        chunk = self.file[start_index:end_index]\n",
        "        # One-hot encode the chosen string\n",
        "        inp = self.char_tensor(chunk[:-1])\n",
        "        # The target string is the same as the\n",
        "        # input string but shifted by 1 character\n",
        "        target = self.char_tensor(chunk[1:])\n",
        "        inp = Variable(inp).cuda()\n",
        "        target = Variable(target).cuda()\n",
        "        return inp, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S7JDlDZRqrg"
      },
      "source": [
        "### LSTM Training loop\n",
        "\n",
        "With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n",
        "especially with the sequence length (chunk_len) used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDVof1Qe1_vG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "batch_size = 256\n",
        "chunk_len = 128\n",
        "model_name = \"LSTM\"\n",
        "train_dataset = LSTMDataset(chunk_len=chunk_len)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, drop_last=True)\n",
        "\n",
        "#Sample parameters, use whatever you see fit.\n",
        "input_dim = train_dataset.n_characters\n",
        "hidden_dim = 256\n",
        "output_dim = train_dataset.n_characters\n",
        "learning_rate = 0.005\n",
        "model = LSTMSimple(chunk_len,input_dim, hidden_dim, output_dim,batch_size)\n",
        "model.train()\n",
        "model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs=30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs = data['input'].float()\n",
        "            labels = data['target'].float()\n",
        "            # b x chunk_len x len(dataset.all_characters)\n",
        "            target = torch.argmax(labels,dim=2)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target.view(labels.shape[0]*labels.shape[1]))\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
        "                                      max_norm=10.0)\n",
        "            optimizer.step()\n",
        "            prog_bar.set_postfix(**{'run:': model_name,'lr': learning_rate,\n",
        "                                    'loss': loss.item()\n",
        "                                    })\n",
        "            prog_bar.update(batch_size)\n",
        "        # Intermediate output\n",
        "        sample_text = \"O Romeo, wherefore art thou\"\n",
        "        inp = train_dataset.char_tensor(sample_text)\n",
        "        sample_input = Variable(inp).cuda().unsqueeze(0).float()\n",
        "        out_test = topk_sampling_lstm(model,sample_input, 300)[0]\n",
        "        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n",
        "        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "        print(\"Top-K sampling -----------------\")\n",
        "        print(out_chars)\n",
        "\n",
        "        out_test = greedy_sampling_lstm(model,sample_input, 300)[0]\n",
        "        out_char_index = torch.argmax(out_test, dim=1).detach().cpu().numpy()\n",
        "        out_chars = sample_text+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "        print(\"Greedy sampling ----------------\")\n",
        "        print(out_chars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwQEM1JHzDo-"
      },
      "source": [
        "# Task 2: Character generation transformer network implementation\n",
        "Our simple transformer-like network will take as input a sequence of characters and predict the next character in the sequence. To ensure an efficient training procedure, masked attention modules will be used as in the [GPT model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
        "\n",
        "For this task you must implement the Scaled dot product attention module and the Masked multi-head attention module. Both of these modules are described in the [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper (See Figure 2 in the paper as well as Sections 3.2.1, 3.2.2 and 3.2.3). They are the core operations of transformers. As we will use our model for text generation also add the masking operation shown as (mask opt.) in Figure 2, implemented as AttentionMasking in the code.\n",
        "\n",
        "**Implement the modules in the ScaledDotProductAttention class and the MultiHeadAttention class.**\n",
        "\n",
        "Read the GPT paper and the Attention is all you need paper for a better understanding of the components. For a more high level overview, this [post](https://jalammar.github.io/illustrated-gpt2/) may also be helpful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNhqnTm8zGRe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super().__init__()\n",
        "        # Positional encoding adds the positional information to the\n",
        "        # embedding. Without it the model would not be able to differentiate\n",
        "        # between different characters orders such as between \"dog\" and \"god\".\n",
        "        position = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div_term = 10000.0**(torch.arange(0,d_model,2).float()/d_model)\n",
        "        print(div_term.shape)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position / div_term)\n",
        "        pe[:, 1::2] = torch.cos(position / div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.pe = pe.cuda()\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.pe[:, :x.size(1)]\n",
        "        return p\n",
        "\n",
        "class AttentionMasking(nn.Module):\n",
        "    def __init__(self, max_len):\n",
        "        super(AttentionMasking, self).__init__()\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_len, max_len))\n",
        "                                     .view(1, 1, max_len, max_len))\n",
        "    def forward(self,x):\n",
        "        length = x.shape[-1]\n",
        "        out = x.masked_fill(self.mask[:,:,:length,:length] == 0, float('-inf'))\n",
        "        return out\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, max_len):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        # Multiply with an upper triangular\n",
        "        # matrix of dimensions (length x length) after the scale operation\n",
        "        # in Figure 2 of the paper.\n",
        "        self.mask_opt = AttentionMasking(max_len)\n",
        "\n",
        "\n",
        "    def forward(self,q,k,v):\n",
        "      # length = number of input tokens\n",
        "      batch_size,num_heads,length,num_neuron = k.size()\n",
        "      # TODO: Implement the scaled dot product attention as described in\n",
        "      # the Attention is all you need paper in Equation 1\n",
        "      pass\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim_model, num_neuron, n_head, max_len):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.num_neuron = num_neuron\n",
        "\n",
        "        # TODO: Initialize the ScaledDotProductAttention and other\n",
        "        # necessary components.\n",
        "\n",
        "    def split(self,tensor):\n",
        "        batch_size, length, total_dim = tensor.size()\n",
        "        # Reshape the tensor to enable the use in\n",
        "        # the ScaledDotProductAttention module\n",
        "        split_tensor = tensor.view(batch_size, length, self.n_head, self.num_neuron).transpose(1,2)\n",
        "        return split_tensor\n",
        "\n",
        "    def concat(self,tensor):\n",
        "        batch_size, num_heads, length, num_neuron = tensor.size()\n",
        "        # Reshape the tensor to its original size before the split operation.\n",
        "        concat_tensor = tensor.transpose(1,2).contiguous().view(batch_size, length, self.n_head*self.num_neuron)\n",
        "        return concat_tensor\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # TODO: Implement the Masked Multi-head attention module as described in the\n",
        "        # Attention is all you need paper in Figure 1 and Section 3.2.2.\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "class PositionFeedForwardNet(nn.Module):\n",
        "    def __init__(self, dim_model):\n",
        "        super(PositionFeedForwardNet, self).__init__()\n",
        "        self.ff_net1 = nn.Linear(dim_model, dim_model*4)\n",
        "        self.ff_net2 = nn.Linear(dim_model*4, dim_model)\n",
        "    def forward(self,x):\n",
        "        ff_out = self.ff_net1(x)\n",
        "        ff_out = torch.nn.functional.relu(ff_out)\n",
        "        ff_out = self.ff_net2(ff_out)\n",
        "        return ff_out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim_model, num_neuron, n_head, max_len):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(dim_model, num_neuron, n_head, max_len)\n",
        "        self.l_norm = torch.nn.LayerNorm(dim_model)\n",
        "        self.l_norm2 = torch.nn.LayerNorm(dim_model)\n",
        "        self.ff_net = PositionFeedForwardNet(dim_model)\n",
        "        # b, len_seq, n_head, num_neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "      # A Transformer block as described in the\n",
        "      # Attention is all you need paper. In Figure 1 the transformer\n",
        "      # block is marked with a gray rectangle right of the text \"Nx\"\n",
        "      _x = x\n",
        "      mha1 = self.mha(x,x,x)\n",
        "      lnorm = self.l_norm(_x+mha1)\n",
        "      _x = lnorm\n",
        "      ff_out = self.ff_net(lnorm)\n",
        "      out = self.l_norm2(ff_out+_x)\n",
        "\n",
        "      return out\n",
        "\n",
        "class TransformerSimple(nn.Module):\n",
        "    def __init__(self, seq_length, input_dim, output_dim,\n",
        "                 batch_size):\n",
        "        super(TransformerSimple, self).__init__()\n",
        "        num_neuron = 64\n",
        "        n_head = 8\n",
        "        dim_model=256\n",
        "        max_len = 512\n",
        "        self.start_embedding = nn.Embedding(input_dim, dim_model)\n",
        "\n",
        "        self.pos_embedding = PositionalEncoding(dim_model)\n",
        "\n",
        "        # b x l x c*n_head\n",
        "        self.t_block1 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block2 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block3 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block4 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "        self.t_block5 = TransformerBlock(dim_model, num_neuron, n_head, max_len)\n",
        "\n",
        "        #self.out_layer_1 = nn.Linear(dim_model, dim_model)\n",
        "        self.output_layer = nn.Linear(dim_model,output_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "      # x - Tensor - (b, seq_len)\n",
        "      # Embeds the input tensor from tokens to features\n",
        "      s_emb = self.start_embedding(x)\n",
        "      # Adds positional embeddings\n",
        "      p_emb = self.pos_embedding(s_emb)\n",
        "      b_out = p_emb + s_emb\n",
        "      # Transformer blocks - You can experiment with varying depth\n",
        "      # For example GPT uses 12 blocks but this might be a bit memory intensive\n",
        "      b_out = self.t_block1(b_out)\n",
        "      b_out = self.t_block2(b_out)\n",
        "      b_out = self.t_block3(b_out)\n",
        "      b_out = self.t_block4(b_out)\n",
        "      b_out = self.t_block5(b_out)\n",
        "\n",
        "      # Output mapping to a classification of output tokens\n",
        "      # For each token the network tries to predict the next token\n",
        "      # based only on the previous tokens.\n",
        "      # Output shape: (b x seq_len x vocabulary_size)\n",
        "      out = self.output_layer(b_out)\n",
        "\n",
        "      return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPjl6ttzPHsq"
      },
      "source": [
        "## Dataset class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T04eYePr8Gn3"
      },
      "outputs": [],
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, chunk_len=200, padded_chunks=False):\n",
        "        # Character based dataset\n",
        "        dataset_path = \"./input.txt\"\n",
        "        # The tokens in the vocabulary (all_characters)\n",
        "        # are just the printable characters of the string class\n",
        "        self.all_characters = string.printable\n",
        "        self.n_characters = len(self.all_characters)\n",
        "        # Maps characters to indices\n",
        "        self.char_dict = {x:i for i,x in enumerate(self.all_characters)}\n",
        "        self.file, self.file_len = self.read_file(dataset_path)\n",
        "        # Sequence length of the input\n",
        "        self.chunk_len = chunk_len\n",
        "        self.encoded_file = [self.char_dict[x] for x in self.file]\n",
        "\n",
        "    def read_file(self,filename):\n",
        "        file = unidecode.unidecode(open(filename).read())\n",
        "        return file, len(file)\n",
        "\n",
        "    def encode_text(self,in_str):\n",
        "        # in_str - input sequence - String\n",
        "        # Returns - in_str mapped to tokens in char_dict\n",
        "        tensor = torch.LongTensor([self.char_dict[x] for x in in_str])\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp, target = self.get_random_text()\n",
        "        return {\"input\":inp, \"target\":target}\n",
        "\n",
        "    def __len__(self):\n",
        "        return 10000\n",
        "\n",
        "    def get_random_text(self):\n",
        "        # Pick a random string of length self.chunk_len from the dataset\n",
        "        start_index = np.random.randint(0, self.file_len - self.chunk_len)\n",
        "        end_index = start_index + self.chunk_len + 1\n",
        "        chunk = self.encoded_file[start_index:end_index]\n",
        "        # input_tokens - random sequence of tokens from the dataset\n",
        "        input_tokens = torch.LongTensor(chunk[:-1])\n",
        "        # target - input token sequence shifted by 1\n",
        "        # the idea is to predict next token for each token in the input sequence\n",
        "        # therefore if the input is [1,2,3,4] the target is [2,3,4,5]\n",
        "        target = torch.LongTensor(chunk[1:])\n",
        "        input_tokens = input_tokens.cuda()\n",
        "        target = target.cuda()\n",
        "        return input_tokens, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhvayVAjsCMf"
      },
      "source": [
        "## Character sampling\n",
        "\n",
        "To generate text the network must predict the next character in a sequence, however networks do not produce a single character but rather estimate the likelihood for each possible character. Sampling characters from the network output can be done in different ways with common ones being the Greedy sampling process and Top-K sampling.\n",
        "\n",
        "In the simple greedy sampling method the network takes a text prompt as input and generates an additional N tokens by always taking the token with the highest prediction score as the next token.\n",
        "\n",
        "In the Top-K sampling, randomness is added to the sampling process as the network samples from K most likely predicitons at each step. This alleviates the problem of generative models repeating text but may generate incorrect text by sampling inappropriate tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IVliOUqqEd5"
      },
      "outputs": [],
      "source": [
        "def topk_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n",
        "    # x -- b x onehot_char\n",
        "    # x = b x l\n",
        "    outputs = torch.zeros((1,num_chars))\n",
        "    inp = x\n",
        "\n",
        "    for t in range(num_chars):\n",
        "        # b x onehot_char\n",
        "        output = model(inp.long())[0,-1:]\n",
        "        #output = torch.softmax(output, dim=1)\n",
        "        # b x 3\n",
        "        output_vals, output_ind = torch.topk(output, 5, dim=1)\n",
        "        # 3 -> int\n",
        "        output_vals = torch.softmax(output_vals, dim=1)\n",
        "        top_ind = torch.multinomial(output_vals[0], 1)[0]\n",
        "        # int\n",
        "        out_char_index = output_ind[0,top_ind]\n",
        "        # int -> 1\n",
        "        out_char_index = torch.ones(1).cuda() * out_char_index\n",
        "\n",
        "        outputs[:,t] = out_char_index.item()\n",
        "        if inp.shape[1] > chunk_len:\n",
        "          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n",
        "        else:\n",
        "          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def greedy_sampling_iter_transformer(model, x, num_chars, chunk_len, output_token):\n",
        "    # x -- shape (batch, tokens in x)\n",
        "    outputs = torch.zeros((1,num_chars))\n",
        "    inp = x\n",
        "\n",
        "    for t in range(num_chars):\n",
        "        # b x l x onehot_char\n",
        "        output = model(inp.long())[0,-1:]\n",
        "        output = torch.softmax(output, dim=1)\n",
        "        out_char_index = torch.argmax(output, dim=1)\n",
        "        outputs[:,t] = out_char_index.item()\n",
        "        if inp.shape[1] > chunk_len:\n",
        "          inp = torch.cat((inp[:,1:], out_char_index.unsqueeze(0)), dim=1)\n",
        "        else:\n",
        "          inp = torch.cat((inp, out_char_index.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6X7-Tlc2iqh"
      },
      "source": [
        "## Transformer model training\n",
        "\n",
        "With a correct implementation you should get sensible text generation results with the set parameters, however you should experiment with various parameters,\n",
        "especially with the sequence length (chunk_len) used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY8aZz1R2g3M"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#Sample parameters, use whatever you see fit.\n",
        "batch_size = 256\n",
        "chunk_len = 128\n",
        "train_dataset = TextDataset(chunk_len=chunk_len)\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "input_dim = train_dataset.n_characters\n",
        "output_dim = train_dataset.n_characters\n",
        "learning_rate = 0.0006\n",
        "\n",
        "model = TransformerSimple(chunk_len, input_dim, output_dim,batch_size)\n",
        "model.train()\n",
        "model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs=50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(total=len(trainloader.dataset), desc ='Training - Epoch: '+str(epoch)+\"/\"+str(epochs), unit='chunks') as prog_bar:\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # inputs - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n",
        "            inputs = data['input'].long()\n",
        "            # labels - shape (batch_size, chunk_len) - Tensor of vocabulary tokens\n",
        "            labels = data['target'].long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            target_t = labels\n",
        "            loss = criterion(outputs.view(inputs.shape[0]*inputs.shape[1],-1),target_t.view(labels.shape[0]*labels.shape[1]))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            prog_bar.set_postfix(**{'run:': \"Transformer\", 'lr': learning_rate,\n",
        "                                    'loss': loss.item()\n",
        "                                    })\n",
        "            prog_bar.update(batch_size)\n",
        "\n",
        "        # Intermediate text output\n",
        "        sample_texts = [\"What authority surfeits on\",\n",
        "                        \"I say unto you, what he hath done famously, he did it to that end:\",\n",
        "                        \"That in submission will return to us: And then, as we have ta'en the sacrament,\"]\n",
        "        output_token = torch.zeros(1,1).cuda()\n",
        "        output_token[0,0] = train_dataset.n_characters-1\n",
        "        print(\"Top-K sampling\")\n",
        "        for sample_text in sample_texts:\n",
        "            sample_encoding = train_dataset.encode_text(sample_text)\n",
        "            sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "\n",
        "            #out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "            out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "            out_char_index = out_test.long().detach().cpu().numpy()\n",
        "            out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "            print(\"----------------------------------------\")\n",
        "            print(out_chars)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text sampling - Transformers\n"
      ],
      "metadata": {
        "id": "Y-Tyjm1kHz5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Here's to my love! O true apothecary! Thy drugs are quick.\"\n",
        "sample_encoding = train_dataset.encode_text(sample_text)\n",
        "sample_input = Variable(sample_encoding).cuda().unsqueeze(0).long()\n",
        "\n",
        "#out_test= greedy_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "out_test= topk_sampling_iter_transformer(model, sample_input, 400, chunk_len, output_token)[0]\n",
        "out_char_index = out_test.long().detach().cpu().numpy()\n",
        "out_chars = sample_text+\" \"+\"\".join([train_dataset.all_characters[i] for i in out_char_index])\n",
        "print(\"----------------------------------------\")\n",
        "print(out_chars)\n"
      ],
      "metadata": {
        "id": "XjvbjEdjH36Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}